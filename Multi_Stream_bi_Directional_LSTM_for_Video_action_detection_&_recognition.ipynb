{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-Stream bi-Directional LSTM for Video action detection & recognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "Eiye5wfj-_Ih",
        "-INlrXJQ_wiA",
        "Ql1XrMyLmo1B",
        "FnLVtbO9F3Tk"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OmerElshrief/Fine-Grained-Action-Detection-Recognition/blob/master/Multi_Stream_bi_Directional_LSTM_for_Video_action_detection_%26_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QCmUv6ITWg8h",
        "colab_type": "text"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpXb6ic2Wddu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torchvision.datasets as dsets\n",
        "from torch.autograd import Variable\n",
        "import numpy as np;\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import math;\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import cv2\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "plt.style.use('grayscale')\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "#print(\"PyTorch Version: \",torch.__version__)\n",
        "#print(\"Torchvision Version: \",torchvision.__version__)\n",
        "import torchvision.models as models\n",
        "from torchsummary import summary\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "import PIL\n",
        "from PIL import Image\n",
        "import scipy\n",
        "import scipy.io\n",
        "import torch.utils.data as utils\n",
        "from itertools import cycle,chain,repeat\n",
        "import itertools\n",
        "import tensorflow as tf\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yDGvmhay_un",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.is_available()\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = tf.Session(config=config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eiye5wfj-_Ih",
        "colab_type": "text"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehxWLzYG-uMV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "# def __init__( start_id , threshold_id, videos_path,labels_path):\n",
        "#   self.subject_id = start_id\n",
        "#   self.threshold = threshold_id\n",
        "#   self.videos_path = videos_path\n",
        "#   self.session_id = 1\n",
        "#   self.labels_path = labels_path\n",
        "\n",
        "\n",
        "\n",
        "def write_video(video,out_path, frame_width, frame_height,subject_id, session_id, as_numpy = False):\n",
        "  if as_numpy:\n",
        "    path = out_path +str(subject_id) + \"_\"+str(session_id)+\".npy\"\n",
        "    np.save(path,np.array(video),allow_pickle=False)\n",
        "  else:\n",
        "\n",
        "    path = out_path +str(subject_id) + \"_\"+str(session_id)+\".avi\"\n",
        "    out = cv2.VideoWriter(path,1, 20.0, frameSize = (frame_width,frame_height),)\n",
        "    print(\"Video lenght: \", len(video))\n",
        "    for frame in video:\n",
        "      # Write the frame into the file 'output.avi'\n",
        "      out.write(frame)\n",
        "    out.release()\n",
        "    del out\n",
        "  print('Video of lenght ',str(len(video)),' is saved at ',path)\n",
        "\n",
        "\n",
        "  \n",
        "def get_data(videos_path,label_path, sample_labels = True,fps = 15, denoising = False, blur = True, resize = True,\n",
        "                normalize = False, frame_width = 256, frame_height=256, background_sub = False, object_trajectory = False, \n",
        "                data_from_numpy = False):\n",
        "  \"\"\"\n",
        "  To load the Dataset, Dataset  can be loaded from Videos or from Stored Numpy arrays \n",
        "  Parameters:\n",
        "  videos_path: Path of the Dataset\n",
        "  Label_path: path of the Labels \n",
        "  sample_labels: If true, labels will be sampled, label per 6 frames \n",
        "  background_sub: If true, frames loaded from videos will be background subtracted (for the Background subtraction models)\n",
        "  object_trajectory: If true, frames loaded from video will be used to extract the Object trajectory for each 6 frames (for Temporal Models)\n",
        "  data_from_numpy: determing if the Dataset is loaded from videos or from Stored Numpy arrays\n",
        "  \"\"\"\n",
        "\n",
        "  data  , length= load_video(videos_path,fps = fps, denoising = denoising,blur = blur,resize = resize,normalize = normalize,frame_width = frame_width,\n",
        "                             frame_height = frame_height,background_sub = background_sub,object_trajectory = object_trajectory,\n",
        "                             data_from_numpy = data_from_numpy )\n",
        "  # print('loaded ',path)\n",
        "  # labels = np.load(labels_path,allow_pickle= True)\n",
        "  labels = get_video_label(label_path,length,half = True,data_from_numpy=data_from_numpy)\n",
        "  if not object_trajectory:\n",
        "    ## We take a sample frame each 6 frames\n",
        "    print(data.shape)\n",
        "    data = data[[i for i in range(0,len(data),6)]]\n",
        "  print(data.shape)\n",
        "  data = [prepare(i) for i in data]\n",
        "  data = torch.stack(data)\n",
        "  print(len(labels))\n",
        "  if sample_labels:\n",
        "    # Since each chunk of video is 6 Frames, we sample the Labels at 6 labels per sample\n",
        "    labels = labels[[i for i in range(0,len(labels),6)]]\n",
        "  ## Transform the Data\n",
        "  labels = torch.tensor(labels)\n",
        "  print(len(labels))\n",
        "  count =len(data)\n",
        "  # print(labels.shape)\n",
        "  # print(data.shape)\n",
        "  ## Sometime labels lenght might not be equal frames lenght,\n",
        "  while (len(labels) > len(data)):\n",
        "    labels = labels[:-1]\n",
        "    print('removed from labels')\n",
        "  while (len(labels) < len(data)):\n",
        "    data = data[:-1]\n",
        "    print('removed from Data')\n",
        "  return data , labels, count\n",
        "\n",
        "\n",
        "def load_video(video_path, fps = 15, denoising = False, blur = True, resize = True, normalize = False, frame_width = 256, frame_height=256, \n",
        "               background_sub = False,object_trajectory = False, data_from_numpy = False):\n",
        "      \"\"\"\n",
        "      background_sub: If true, frames loaded from videos will be background subtracted (for the Background subtraction models)\n",
        "      object_trajectory: If true, frames loaded from video will be used to extract the Object trajectory for each 6 frames\n",
        "      data_from_numpy: determing if the Dataset is loaded from videos or from Stored Numpy arrays\n",
        "      \"\"\"\n",
        "      video  = []\n",
        "      print('Loading ',video_path)\n",
        "      if data_from_numpy:\n",
        "         video = np.load(path)\n",
        "         length = len(video)\n",
        "\n",
        "      else:\n",
        "          cap = cv2.VideoCapture(video_path)\n",
        "          length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "          \n",
        "          # taking the Background from 2nd second \n",
        "          cap.set(cv2.CAP_PROP_POS_MSEC,2*1000)\n",
        "          reval, background = cap.read()\n",
        "          # Preprocess the Background\n",
        "          background = cv2.GaussianBlur(background, (3,3), 0) \n",
        "          # Get back to the 0 second\n",
        "          cap.set(cv2.CAP_PROP_POS_MSEC,0)\n",
        "          while True:\n",
        "#                 video.append(img)\n",
        "              if object_trajectory: # If it's desired to Load video frames and get the object trajectory for each 6 consecutive frames\n",
        "                 chunk = []\n",
        "                 for i in range(0,6):\n",
        "                    reval, img = cap.read() # This is neglected because we load at 15 FPS , and video is stored at 30 FPS\n",
        "                    reval, img = cap.read()\n",
        "\n",
        "                    if not reval:\n",
        "                        # End of frames\n",
        "                        print('END')\n",
        "                        video = np.array(video)\n",
        "                        return video , length\n",
        "                    if background_sub:      # This is needed for the 4th Model, as we subtract the BG and then get OT\n",
        "                        # Subtract Background\n",
        "                        img = subtract_background(img,background)\n",
        "                    img = cv2.resize(img, (512, 512))\n",
        "                    chunk.append(img) # Here we store video chunk of 6 Frames and then pass it to get_stacked_pixel_trajectory\n",
        "                 chunk = np.array(chunk)\n",
        "                 video.append(get_stacked_pixel_trajectory(chunk,2))\n",
        "\n",
        "              else:\n",
        "\n",
        "                  reval, img = cap.read() # Video is stored at 30 FPS, we want to load it at 15 FPS so we ignore a frame each iteration\n",
        "                  reval, img = cap.read() # Faster than cap.set()\n",
        "\n",
        "                  if not reval:\n",
        "                      # End of frames\n",
        "                      break\n",
        "                  if background_sub:\n",
        "                    # Subtract Background\n",
        "                    img = subtract_background(img,background)\n",
        "\n",
        "                  if resize:\n",
        "                    img = cv2.resize(img, (frame_width, frame_height)) \n",
        "                  if blur:\n",
        "                    img = cv2.GaussianBlur(img, (21,21), 0) \n",
        "                  if normalize:\n",
        "                    img = cv2.normalize(img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "                  \n",
        "                  video.append(img)\n",
        "\n",
        "          video = np.array(video)\n",
        "          # try:\n",
        "          #   video = video[[i for i in range(0,len(video),2)]]\n",
        "          # except IndexError:\n",
        "            # print('index error')\n",
        "            # pass\n",
        "          cap.release()\n",
        "          del cap\n",
        "          print(length)\n",
        "      return video , length\n",
        "\n",
        "    \n",
        "def get_video_label(label_path,video_lenght, half = True, data_from_numpy = False):\n",
        "  \"\"\"\n",
        "  Load the labels from matlab files or form Numpy files\n",
        "  \"\"\"\n",
        "  if data_from_numpy :\n",
        "    labels = np.load(path)\n",
        "\n",
        "  else:\n",
        "\n",
        "    label_data = scipy.io.loadmat(label_path)\n",
        "    # Initially each frame is label 0 \n",
        "    label = [0 for i in range(video_lenght)] \n",
        "    label = np.array(label)\n",
        "\n",
        "    for category_number in range(5):\n",
        "        # Each video chunk in the same class defined as in labels\n",
        "        for video_chunk in label_data['tlabs'][category_number][0]:\n",
        "          label[video_chunk[0]:video_chunk[1]] = category_number+1\n",
        "\n",
        "    label = np.array(label)\n",
        "    if half:\n",
        "        label = label[[i for i in range(1,video_lenght,2)]]\n",
        "\n",
        "  return (label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlTW1gIOh1yn",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLVHin6-_TYr",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zwmEXU5_VEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    \n",
        "    [\n",
        "#         transforms.ToPILImage(),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "     ])\n",
        "\n",
        "\n",
        "def prepare(img):\n",
        "  \n",
        "  # img = cv2.fastNlMeansDenoisingColored(img,None,15,10,7,21)\n",
        "  # img = cv2.GaussianBlur(img, (3,3), 0) \n",
        "  img = transform(img)\n",
        "  \n",
        "  # img = cv2.normalize(img, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "  \n",
        "  return img\n",
        "  \n",
        "def get_optical_flow(first_frame,second_frame, rgb ):\n",
        "        '''\n",
        "        To extract the Obtical flow between 2 frames \n",
        "        '''\n",
        "        # Converts frame to grayscale because we only need the luminance channel for detecting edges - less computationally expensive\n",
        "        original = first_frame\n",
        "        if rgb:\n",
        "         \n",
        "          first_frame = cv2.cvtColor(first_frame, cv2.COLOR_BGR2GRAY)\n",
        "          second_frame = cv2.cvtColor(second_frame, cv2.COLOR_BGR2GRAY)\n",
        "        else:\n",
        "           original = cv2.cvtColor(original, cv2.COLOR_GRAY2BGR)\n",
        "        # denoising and bluring\n",
        "        # first_gray = cv2.fastNlMeansDenoising(first_gray,None,templateWindowSize= 7,h = 10,searchWindowSize  = 21)\n",
        "        \n",
        "        # second_gray = cv2.fastNlMeansDenoising(second_gray,None,templateWindowSize= 7,h = 10,searchWindowSize  = 21)\n",
        "        # Creates an image filled with zero intensities with the same dimensions as the frame\n",
        "        mask = np.zeros_like(original)\n",
        "        # Sets image saturation to maximum\n",
        "        mask[..., 1] = 255\n",
        "        flow = cv2.calcOpticalFlowFarneback(first_frame, second_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "        # Computes the magnitude and angle of the 2D vectors\n",
        "        magnitude, angle = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "        # Sets image hue according to the optical flow direction\n",
        "        mask[..., 0] = angle * 180 / np.pi / 2\n",
        "        # Sets image value according to the optical flow magnitude (normalized)\n",
        "        mask[..., 2] = cv2.normalize(magnitude, None, 0, 255, cv2.NORM_MINMAX)\n",
        "        # Converts HSV to RGB (BGR) color representation\n",
        "        rgb = cv2.cvtColor(mask, cv2.COLOR_HSV2BGR)\n",
        "        return cv2.resize(rgb, (224, 224))\n",
        "\n",
        "\n",
        "  \n",
        "def get_stacked_pixel_trajectory( video1, i,rgb = True):\n",
        "    '''\n",
        "    Function to get stacked optical flows between a Middle from video[i] and it's 6 neighbouring frames\n",
        "    to form the Object trajectory \n",
        "    video: chunk of frames [7 frames]\n",
        "    i : index of the middel frame\n",
        "    '''\n",
        "#     print(\"Extracting Obejct trajectory\")\n",
        "    of1 = get_optical_flow(first_frame = video1[i], second_frame=video1[i-3],rgb = rgb )\n",
        "    of2 = get_optical_flow(video1[i], video1[i-2],rgb)\n",
        "    of3 = get_optical_flow(video1[i],video1[i-1],rgb)\n",
        "\n",
        "    of4 = get_optical_flow(video1[i],video1[i+1],rgb)\n",
        "    of5 = get_optical_flow(video1[i],video1[i+2],rgb)\n",
        "    of6 = get_optical_flow(video1[i],video1[i+3],rgb)\n",
        "\n",
        "    of = np.concatenate((of1, of2, of3, of4, of5, of6), axis = 1)\n",
        "#     plt.figure()\n",
        "#    plt.imshow(of)\n",
        "    return of\n",
        "\n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "def subtract_background(frame,background, List = False):\n",
        "    \n",
        "    if List:\n",
        "      chunk = []\n",
        "      for img in frame:\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        # In each iteration, calculate absolute difference between current frame and reference frame\n",
        "        difference = cv2.absdiff(gray, background)\n",
        "        # Apply thresholding to eliminate noise\n",
        "        thresh = cv2.threshold(difference, 15, 255, cv2.THRESH_BINARY)[1]\n",
        "        thresh = cv2.dilate(thresh, None, iterations=2)\n",
        "        thresh = cv2.cvtColor(thresh,cv2.COLOR_GRAY2BGR)\n",
        "        chunk.append(thresh)\n",
        "      return chunk\n",
        "\n",
        "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    # In each iteration, calculate absolute difference between current frame and reference frame\n",
        "    difference = cv2.absdiff(gray, background)\n",
        "    # Apply thresholding to eliminate noise\n",
        "    thresh = cv2.threshold(difference, 15, 255, cv2.THRESH_BINARY)[1]\n",
        "    thresh = cv2.dilate(thresh, None, iterations=2)\n",
        "    thresh = cv2.cvtColor(thresh,cv2.COLOR_GRAY2BGR)\n",
        "    return(thresh)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def label_to_one_hot(labels):\n",
        "    batch_size = len(labels)\n",
        "    nb_digits = 6\n",
        "    # Dummy input that HAS to be 2D for the scatter (you can use view(-1,1) if needed)\n",
        "    y = torch.Tensor(labels.float()).view(-1,1).long()\n",
        "    # One hot encoding buffer that you create out of the loop and just keep reusing\n",
        "    y_onehot = torch.FloatTensor(batch_size, nb_digits)\n",
        "\n",
        "    # In your for loop\n",
        "    y_onehot.zero_()\n",
        "    y_onehot.scatter_(1, y, 1)\n",
        "\n",
        "    return y_onehot\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-INlrXJQ_wiA",
        "colab_type": "text"
      },
      "source": [
        "## Evaluation Functions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCkop4su_yIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.metrics import average_precision_score\n",
        "from itertools import cycle\n",
        "\n",
        "\n",
        "def evaluate_average_percision(Y_test, y_score, n_classes):\n",
        "    # For each class\n",
        "    precision = dict()\n",
        "    recall = dict()\n",
        "    average_precision = dict()\n",
        "    for i in range(n_classes):\n",
        "        precision[i], recall[i], _ = precision_recall_curve(Y_test[:, i],\n",
        "                                                            y_score[:, i])\n",
        "        average_precision[i] = average_precision_score(Y_test[:, i], y_score[:, i])\n",
        "\n",
        "    # A \"micro-average\": quantifying score on all classes jointly\n",
        "    precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(),\n",
        "        y_score.ravel())\n",
        "    average_precision[\"micro\"] = average_precision_score(Y_test, y_score,\n",
        "                                                         average=\"micro\")\n",
        "    return precision,recall,average_precision\n",
        "    \n",
        "def print_Average_precision(precision, recall, average_precision, n_classes, plot_classes = False):\n",
        "    \n",
        "    print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
        "          .format(average_precision[\"micro\"]))\n",
        "\n",
        "    plt.figure()\n",
        "    plt.step(recall['micro'], precision['micro'], color='b', alpha=0.2,\n",
        "             where='post')\n",
        "    plt.fill_between(recall[\"micro\"], precision[\"micro\"], alpha=0.2, color='b'    )\n",
        "\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.title(\n",
        "        'Average precision score, micro-averaged over all classes: AP={0:0.2f}'\n",
        "        .format(average_precision[\"micro\"]))\n",
        "    \n",
        "    if plot_classes:\n",
        "      colors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal','teal'])\n",
        "      plt.figure(figsize=(7, 8))\n",
        "      f_scores = np.linspace(0.2, 0.8, num=4)\n",
        "      lines = []\n",
        "      labels = []\n",
        "      for f_score in f_scores:\n",
        "          x = np.linspace(0.01, 1)\n",
        "          y = f_score * x / (2 * x - f_score)\n",
        "          l, = plt.plot(x[y >= 0], y[y >= 0], color='gray', alpha=0.2)\n",
        "          plt.annotate('f1={0:0.1f}'.format(f_score), xy=(0.9, y[45] + 0.02))\n",
        "\n",
        "      lines.append(l)\n",
        "      labels.append('iso-f1 curves')\n",
        "      l, = plt.plot(recall[\"micro\"], precision[\"micro\"], color='gold', lw=2)\n",
        "      lines.append(l)\n",
        "      labels.append('micro-average Precision-recall (area = {0:0.2f})'\n",
        "                    ''.format(average_precision[\"micro\"]))\n",
        "\n",
        "      for i, color in zip(range(n_classes), colors):\n",
        "          l, = plt.plot(recall[i], precision[i], color=color, lw=2)\n",
        "          lines.append(l)\n",
        "          labels.append('Precision-recall for class {0} (area = {1:0.2f})'\n",
        "                        ''.format(i, average_precision[i]))\n",
        "\n",
        "      fig = plt.gcf()\n",
        "      fig.subplots_adjust(bottom=0.25)\n",
        "      plt.xlim([0.0, 1.0])\n",
        "      plt.ylim([0.0, 1.05])\n",
        "      plt.xlabel('Recall')\n",
        "      plt.ylabel('Precision')\n",
        "      plt.title('Extension of Precision-Recall curve to multi-class')\n",
        "      plt.legend(lines, labels, loc=(0, -.38), prop=dict(size=14))\n",
        "      \n",
        "      \n",
        "def evaluation(on_hot_predictions, ground_truth):\n",
        "  \"\"\"\"\n",
        "  input: one hot encoded predictions, ground truth labels \n",
        "  output: list of 5 precisions for each class \n",
        "  \"\"\"\"\"\n",
        "  \n",
        "  predictions = np.argmax(on_hot_predictions, axis = 1)\n",
        "  pred = []\n",
        "  for idx, _ in enumerate(predictions): \n",
        "    pred.append(np.dot(np.where(predictions == idx), 6))\n",
        "  \n",
        "  labels = []\n",
        "  for category in ground_truth:\n",
        "    labels.append(list(map(lambda x: list(range(x[0],x[1])), category[0])))\n",
        "  \n",
        "  true = []\n",
        "  for idx, _ in enumerate(ground_truth):\n",
        "    true.append(list(map(lambda x: np.sum(np.isin(pred[idx][0], x)), labels[idx])))\n",
        "  \n",
        "  true_positives = []\n",
        "  for idx, _ in enumerate(ground_truth):\n",
        "    true_positives.append(np.sum(np.array(true[idx]) != 0))\n",
        "    \n",
        "  false_positives = []\n",
        "  for idx, _ in enumerate(ground_truth):\n",
        "    false_positives.append(len(pred[idx][0]) - true_positives[idx])\n",
        "  \n",
        "  precison = np.array(true_positives) / (np.array(true_positives) + np.array(false_positives))\n",
        "  return precison \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfdV6ZbCigUo",
        "colab_type": "text"
      },
      "source": [
        "## Avidbeam Evaluation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOUheoBaie1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is an implementation of the mAP metric calculation over classes 1-5 (ignoring class 0)\n",
        "\n",
        "def evaluate(result, gt):\n",
        "  assert(len(result) == len(gt)), 'input arrays must have the same size'\n",
        "  \n",
        "  # saving true positives (tp) counts and false positives (fp) counts\n",
        "  tp = {'1' : 0, '2' : 0, '3' : 0, '4' : 0, '5' : 0}\n",
        "  fp = {'1' : 0, '2' : 0, '3' : 0, '4' : 0, '5' : 0}\n",
        "  \n",
        "  for index in range(0, len(gt)):\n",
        "    # ignoring class 0\n",
        "    if gt[index] != 0 and result[index] != 0:\n",
        "      if gt[index] == result[index]:\n",
        "        tp[str(gt[index])] += 1\n",
        "      else:\n",
        "        fp[str(result[index])] += 1\n",
        "\n",
        "  # calculating precision per class as tp/(tp + fp)\n",
        "  AP = {k : tp[k]/(tp[k] + fp[k]) for k in tp if k in fp}\n",
        "  print ('precision per class =', AP)\n",
        "  mAP = float(sum(AP.values())) / len(AP)\n",
        "  print ('mAP =', mAP)\n",
        "  return mAP\n",
        "      \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVN6ifw9AARx",
        "colab_type": "text"
      },
      "source": [
        "## Helper Fucntions "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhK26gfjABhy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    if feature_extracting:\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "    else:\n",
        "      for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "      \n",
        "    \n",
        "      \n",
        "def create_optimizer(model , r = 0.0001):\n",
        "  params_to_update = model.parameters()\n",
        "  print(\"Params to learn:\")\n",
        "  \n",
        "  params_to_update = []\n",
        "  for name,param in model.named_parameters():\n",
        "        if param.requires_grad == True:\n",
        "            params_to_update.append(param)\n",
        "            print(\"\\t\",name)\n",
        "        \n",
        "  return  optim.SGD(params_to_update, lr=r,weight_decay=1e-6)\n",
        "\n",
        "      \n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, input):\n",
        "        return input.view(input.size(0), -1)\n",
        "\n",
        "class Identity(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Identity, self).__init__()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        return x\n",
        "      \n",
        "class Transform(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Transform, self).__init__()\n",
        "  \n",
        "  def forward(self,x):\n",
        "    return transforms(x)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdF13hfQAFOJ",
        "colab_type": "text"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOzAdPQoW3K9",
        "colab_type": "code",
        "outputId": "d1416bcb-d928-4a2a-cf0e-bd5645640da3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "import torchvision.models as models\n",
        "vgg16 = models.vgg16(pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 553433881/553433881 [00:09<00:00, 58894800.29it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43bHEsGtAErw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def prepare_models(spatial,temporal,spatial2,temporal2,path_to_weights_dir):\n",
        "  weights = torch.load(path_to_weights_dir+\"/model1.pi\")\n",
        "  spatial.load_state_dict(weights)\n",
        "  weights = torch.load(path_to_weights_dir+\"/model_temporal.pt\")\n",
        "  temporal.load_state_dict(weights)\n",
        "#   weights = torch.load(path_to_weights_dir+\"/sub_back_patial_model12.pi\")\n",
        "#   spatial2.load_state_dict(weights)\n",
        "#   weights= torch.load(path_to_weights_dir+\"/model_temporal_background_subtracted1.pt\")\n",
        "#   temporal2.load_state_dict(weights)\n",
        "  del weights\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "  return spatial,temporal\n",
        "\n",
        "\n",
        "class backbone_feature_extractor (nn.Module):\n",
        "  def __init__(self,features_extraction_output,softmax_output = False, classification = False,  Dropout = 0.5):\n",
        "    \n",
        "        super().__init__()\n",
        "        \n",
        "        self.features_extractor = vgg16.features\n",
        "        set_parameter_requires_grad(self.features_extractor,True)\n",
        "#         set_parameter_requires_grad(self.features_extractor[24:31],False)\n",
        "        self.flatten = Flatten()\n",
        "        self.classifier = nn.Sequential(nn.Linear(in_features = features_extraction_output ,out_features=4096, bias=True ),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Dropout(Dropout),\n",
        "                                        nn.Linear(in_features=4096, out_features=1024, bias=True),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Dropout(Dropout),\n",
        "                                        nn.Linear(in_features=1024, out_features=6, bias=True)\n",
        "                                       )\n",
        "        set_parameter_requires_grad(self.classifier,True)                                \n",
        "        self.classification = classification\n",
        "  \n",
        "  def forward(self, x):\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    x = self.features_extractor(x)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    if self.classification:\n",
        "      \n",
        "      x= self.classifier(x)\n",
        "      \n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class MSN(nn.Module):\n",
        "  \n",
        "  \"\"\"\n",
        "  Takes a chunked video of shape (n_chunks, frames, frame_height, frame_width, channels)\n",
        "  \n",
        "  Outputs a tensor of shape (n_chunks, 200) containing chunk feature vector\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self,spatial,temporal,spatial2,temporal2,features_extraction_output):\n",
        "    super().__init__()\n",
        "#     weights = torch.load(path_to_weights_dir+\"/model_temporal.pt\")\n",
        "#     keys = ['classifier.0.weight', 'classifier.0.bias', 'classifier.3.weight', 'classifier.3.bias', 'classifier.6.weight','classifier.6.bias']\n",
        "#     for key in keys:\n",
        "#     weights.pop(key)  ## We only need Feature extractors weights\n",
        "    \n",
        "    self.cnn_temp_coarse = temporal2\n",
        "#     self.cnn_temp_fine   = temporal2\n",
        "    self.cnn_spat_coarse = spatial\n",
        "#     self.cnn_spat_fine   = spatial\n",
        "\n",
        "    self.fin_vec = nn.Sequential(nn.Linear(in_features =features_extraction_output , out_features = 2048),\n",
        "                                 nn.ReLU(),\n",
        "                                 nn.Dropout(0.5),\n",
        "                                 nn.Linear(in_features = 2048 , out_features= 200)\n",
        "                                )\n",
        "   \n",
        "                                 \n",
        "    self.flatten = Flatten()\n",
        "    \n",
        "  def forward(self,x):\n",
        "    \n",
        "    x1 = (self.cnn_spat_coarse(x[1]))\n",
        "    x2 = (self.cnn_temp_coarse(x[2]))\n",
        "#     x3 = (self.cnn_spat_fine(x[3]))\n",
        "#     x4 = (self.cnn_temp_fine(x[4]))\n",
        "    o = torch.cat((x1, x2), dim=1)\n",
        "    \n",
        "    o = self.flatten(o)\n",
        "    torch.cuda.empty_cache()\n",
        "    o = self.fin_vec(o)\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    return o\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class bi_lstm(nn.Module):\n",
        "\n",
        "  def __init__(self,msn,batch_size,hidden_size=60,num_layers = 1):\n",
        "    super().__init__()\n",
        "    self.msn_feature_extractor = msn\n",
        "    self.lstm = nn.LSTM(200, hidden_size= hidden_size, num_layers=num_layers, batch_first=True, bidirectional=True )\n",
        "\n",
        "#     self.lstm.cuda()\n",
        "    \n",
        "    # h_0, c_0 of shape (num_layers * num_directions, batch, hidden_size)\n",
        "    \n",
        "    self.hidden_vect_1 = hidden_vect_1 = (Variable(torch.zeros(2*num_layers, batch_size, hidden_size).cuda()),\n",
        "                                          Variable(torch.zeros(2*num_layers, batch_size, hidden_size).cuda()))\n",
        "    self.forward_classifier = nn.Linear(in_features = 60, out_features = 6)\n",
        "    self.backward_classifier = nn.Linear(in_features = 60, out_features = 6)\n",
        "\n",
        "  def forward(self,x):\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    x = self.msn_feature_extractor(x)   ## Shape = (n_chunks, 200)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    x = x.view(-1, 1, 200)\n",
        "    \n",
        "    # output of shape (seq_len, batch, num_directions * hidden_size)\n",
        "    output, hidden = self.lstm( x, self.hidden_vect_1    )\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    x_forward, x_backward = torch.split(output,60,dim=2)\n",
        "  \n",
        "  \n",
        "    x_forward= (self.forward_classifier(x_forward))\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    x_backward = (self.backward_classifier(x_backward))\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    x = (x_forward+x_backward) /2\n",
        "    \n",
        "\n",
        "    return (x)\n",
        "\n",
        "  \n",
        "  \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ql1XrMyLmo1B",
        "colab_type": "text"
      },
      "source": [
        "## Manual cells LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSa58y1-mojl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(nn.Module):\n",
        "  \n",
        "  \"\"\"\n",
        "  Takes a chunked video of shape (n_chunks, frames, channels, frame_height, frame_width)\n",
        "                         \n",
        "  Outputs 2 tensors of shape (n_chunks, 2) labels containing the action prediction of chunks\n",
        "  using backward and forward lstm\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self,msn):\n",
        "    super().__init__()\n",
        "    self.msn_feature_extractor = msn\n",
        "    self.lstm_cell = nn.LSTM(input_size = 200, hidden_size = 60,num_layers=1, batch_first=False,bidirectional= True)\n",
        "    self.classifier = nn.Linear(in_features = 60, out_features = 6)\n",
        "    self.fin = nn.Softmax(dim = 1)\n",
        "    \n",
        "    \n",
        "  def forward(self,x, get_logits = True):\n",
        "    x = self.msn_feature_extractor(x)   ## Shape = (n_chunks, 200)\n",
        "    x = x.view(-1, 1, 200)\n",
        "    \n",
        "    hx = torch.randn(1, 60)                ## Handle shape\n",
        "    cx = torch.randn(1, 60)                ## Handle shape\n",
        "    forward_outputs = []\n",
        "    print(x.shape)\n",
        "    # for chunk in range(x.shape[0]):\n",
        "    hx, cx = self.lstm_cell(x, (hx, cx))\n",
        "    forward_outputs.append(hx)\n",
        "      \n",
        "    hx = torch.randn(1, 60)                ## Handle shape\n",
        "    cx = torch.randn(1, 60)                ## Handle shape   \n",
        "    backward_outputs = []\n",
        "    # for chunk in reversed(range(x.shape[0])):\n",
        "    hx, cx = self.lstm_cell(x, (hx, cx))\n",
        "    backward_outputs.append(hx)\n",
        "      \n",
        "    forward_outputs = torch.FloatTensor(forward_outputs)      ## Handle Shape\n",
        "    backward_outputs = torch.FloatTensor(backward_outpus)     ## Handle Shape\n",
        "    \n",
        "    pre_logits_forw = self.classifier(forward_outputs)\n",
        "    pre_logits_back = self.classifier(backward_outputs)\n",
        "    \n",
        "    if get_logits:\n",
        "      logits_forw = self.fin(pre_logits_forw)\n",
        "      logits_back = self.fin(pre_logits_back)\n",
        "      return torch.mean(torch.stack([logits_forw, logits_back])) \n",
        "    \n",
        "    return pre_logits_forw, pre_logits_back   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnLVtbO9F3Tk",
        "colab_type": "text"
      },
      "source": [
        "## Training & Testing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pU6QWD3gF5BS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def model_fit(data_path, label_path, from_numpy, model_type, no_training_subjects, no_validation_subjects,\n",
        "          batch_size,classification_model,save_path  ,\n",
        "          optimizer, no_epochs=5 ,fps=15 ,criterion =  nn.CrossEntropyLoss()):\n",
        "    '''\n",
        "    This function performs the following\n",
        "    1- load the Dataset (call get_data) according to the Type of model and Type of Data set (NUMPY of Video)\n",
        "    2- Construct the Data Loader \n",
        "    3- Perform training step (call train function)\n",
        "    4- Perform validation step\n",
        "    Parameters:\n",
        "    from_numpy (boolean) if true, the data is loaded from numpy arrays\n",
        "    '''\n",
        "    val_acc = -999\n",
        "    history = []\n",
        "    validation_subjects = no_training_subjects+1 + no_validation_subjects\n",
        "    \n",
        "    if from_numpy:\n",
        "      data_extension = '.npy'\n",
        "      label_extension = '_label.npy'\n",
        "      \n",
        "    else:\n",
        "      data_extension = '_crop.mp4'\n",
        "      label_extension = '_label.mat'\n",
        "      if model_type ==3:\n",
        "        background_sub = True\n",
        "   \n",
        "         \n",
        "           \n",
        "        \n",
        "    for i in range(no_epochs):\n",
        "        print('Training Epoch {} ...'.format(i+1))\n",
        "        running_loss,train_correct,valid_correct,val_loss = 0,0,0,0\n",
        "        session_id, subject_id = 1,1\n",
        "        train_count,valid_count = 0,0\n",
        "        while subject_id < no_training_subjects+1:\n",
        "\n",
        "            # Loading the videos and labels one at a time from the stored Numpy files\n",
        "            path =  data_path + \"/\"+ str(subject_id) + \"_\" + str(session_id)+ data_extension\n",
        "            if  os.path.exists(path): # check whether this file exists in the directory\n",
        "              labels_path =  label_path + str(subject_id) +\"_\" + str(session_id)+label_extension\n",
        "              \n",
        "              data , labels, count = get_data(path,labels_path,True , background_sub = background_sub,data_from_numpy = from_numpy)\n",
        "              train_count += count\n",
        "              print(data.shape)\n",
        "              print(labels.shape)\n",
        "              my_dataset = utils.TensorDataset(data,labels) # create your datset\n",
        "              train_dataloader = utils.DataLoader(my_dataset,batch_size=batch_size, shuffle=False) # \n",
        "              del data, labels, my_dataset # To free some CUDA Memory\n",
        "              # Training Step\n",
        "              loss , correct = training_batch(classification_model,train_dataloader,optimizer,criterion)\n",
        "              running_loss +=loss\n",
        "              train_correct += correct\n",
        "            else:\n",
        "                print('No such path:' ,path)\n",
        "            # Next Video    \n",
        "            session_id += 1\n",
        "            if session_id > 3:\n",
        "              subject_id += 1\n",
        "              session_id = 1\n",
        "           \n",
        "        ## Validation step\n",
        "        print( \" Validating Epoch \", i+1)\n",
        "        subject_id,session_id = no_training_subjects+1 , 1\n",
        "        \n",
        "        while subject_id < validation_subjects:\n",
        "\n",
        "            # Loading the videos and labels one at a time from the stored Numpy files or from Stored Videos\n",
        "            path =  data_path + \"/\"+ str(subject_id) + \"_\" + str(session_id)+\"_crop.mp4\"\n",
        "            if  os.path.exists(path): # check whether this file exists in the directory\n",
        "              labels_path =  label_path + str(subject_id) +\"_\" + str(session_id)+\"_label.mat\"\n",
        "              \n",
        "              data , labels, count = get_data(path,labels_path,True , background_sub = background_sub,data_from_numpy = from_numpy)\n",
        "              valid_count += count\n",
        "              my_dataset = utils.TensorDataset(data,labels) # create your datset\n",
        "              train_dataloader = utils.DataLoader(my_dataset,batch_size=batch_size, shuffle=False) # \n",
        "              del data, labels, my_dataset # To free some CUDA Memory\n",
        "              # Validation Step\n",
        "              loss , correct = validation_step(classification_model,train_dataloader,criterion)\n",
        "              val_loss += loss\n",
        "              valid_correct += correct\n",
        "            # Next Video\n",
        "            session_id += 1\n",
        "            if session_id > 3:\n",
        "              subject_id += 1\n",
        "              session_id = 1\n",
        "        \n",
        "        test_accu =  int(train_correct) / train_count\n",
        "        valid_accu = int(valid_correct) / valid_count\n",
        "\n",
        "        print('Train correct: ',int(train_correct), ' out of: ',train_count )\n",
        "        print('Tets correct: ', int(valid_correct), ' out of: ',valid_count)\n",
        "        print ('Epoch [{}/{}], -T-Loss : {:.6f} , Train_accuracy: {:.4f} ,  Val_loss: {:.6f}  -Val_acc: {:.4f}'.format(\n",
        "            i+1, no_epochs,running_loss/train_count,test_accu ,val_loss/valid_count, valid_accu))\n",
        "\n",
        "        if val_acc < valid_accu:\n",
        "          torch.save(classification_model.state_dict(), save_path)\n",
        "          print('val_acc has improved from {:.4f} to {:.4f}, model is saved at {}'.format(val_acc , valid_accu,save_path))\n",
        "          val_acc = valid_accu\n",
        "        else:\n",
        "          print('val_acc has not Improved from {:.4f}'.format(val_acc))    \n",
        "        history.append([i,running_loss/train_count,valid_accu,val_loss/valid_count])\n",
        "\n",
        "        #accu.append(100 * int(test_correct) / len(X_test))\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "\n",
        "def training_batch(classification_model,train_dataloader,optimizer,criterion):\n",
        "      running_loss,train_correct = 0,0\n",
        "      probability = nn.Softmax()\n",
        "      for batch,y_true in train_dataloader:\n",
        "          y_true= y_true.type(torch.LongTensor)\n",
        "          y_true = y_true.cuda()\n",
        "          batch = (batch.cuda())\n",
        "          optimizer.zero_grad()\n",
        "          y_pred = classification_model(batch)\n",
        "          loss = criterion(y_pred, y_true)\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          running_loss += loss.item()\n",
        "          y_pred = probability(y_pred)\n",
        "          train_correct +=  (y_true == y_pred.max(1)[1]).sum()\n",
        "          del loss, batch\n",
        "          y_pred.cpu()\n",
        "          y_true.cpu()\n",
        "          torch.cuda.empty_cache()\n",
        "      return running_loss,train_correct\n",
        "\n",
        "def validation_step(classification_model,train_dataloader,criterion):\n",
        "  val_loss,valid_correct = 0,0\n",
        "  for batch,y_true in train_dataloader:\n",
        "      y_true= y_true.type(torch.LongTensor)\n",
        "      y_true = y_true.cuda()\n",
        "      batch = (batch.cuda())\n",
        "      with torch.no_grad():\n",
        "        y_pred = classification_model(batch)\n",
        "        y_pred = y_pred.view(-1,6)\n",
        "        valid_correct +=  (y_true == y_pred.max(1)[1]).sum()\n",
        "        loss = criterion(y_pred, y_true)\n",
        "        val_loss += loss.item()\n",
        "        del batch\n",
        "        y_pred.cpu()\n",
        "        y_true.cpu()\n",
        "        torch.cuda.empty_cache()\n",
        "  return val_loss, valid_correct\n",
        "\n",
        "\n",
        "def test(classification_model, model_type, data_path, label_path, data_from_numpy,subject_id, test_subjects, batch_size,\n",
        "         background_sub = False, criterion =  nn.CrossEntropyLoss()):\n",
        "    '''\n",
        "    Parameters:\n",
        "    subject_id: initial subject_id, the starting point\n",
        "    test_subjects: Number of test subjects\n",
        "    '''\n",
        "    probability = nn.Softmax()\n",
        "    test_count,valid_correct,valid_count = 0,0,0\n",
        "    threshold = subject_id + test_subjects\n",
        "    session_id = 1\n",
        "    while subject_id < threshold:\n",
        "\n",
        "        # Loading the videos and labels one at a time from the stored Numpy files\n",
        "        # path = data_path + str(subject_id)+\"_\"+str(session_id)+\"_fps_\"+str(fps)+'.npy'\n",
        "       # Loading the videos and labels one at a time from the stored Numpy files or from Stored Videos\n",
        "        path =  data_path + \"/\"+ str(subject_id) + \"_\" + str(session_id)+\"_crop.mp4\"\n",
        "        if  os.path.exists(path): # check whether this file exists in the directory\n",
        "          labels_path =  label_path + str(subject_id) +\"_\" + str(session_id)+\"_label.mat\"\n",
        "\n",
        "          data , labels, count = get_data(path,labels_path,True , background_sub = background_sub,data_from_numpy = from_numpy)\n",
        "          test_count += count\n",
        "          my_dataset = utils.TensorDataset(data,labels) # create your datset\n",
        "          train_dataloader = utils.DataLoader(my_dataset,batch_size=batch_size, shuffle=False) # \n",
        "          del data, labels, my_dataset # To free some CUDA Memory\n",
        "\n",
        "          score = []\n",
        "          predictions =[]\n",
        "          for batch,y_true in train_dataloader:\n",
        "              y_true= y_true.type(torch.LongTensor)\n",
        "              y_true = y_true.cuda()\n",
        "              batch = (batch.cuda())\n",
        "              with torch.no_grad():\n",
        "                y_pred = classification_model(batch)\n",
        "                y_pred = y_pred.view(-1,6)\n",
        "\n",
        "                valid_correct +=  (y_true == y_pred.max(1)[1]).sum()\n",
        "                loss = criterion(y_pred, y_true)\n",
        "                val_loss += loss.item()\n",
        "                y_pred = probability(y_pred)\n",
        "                y_pred = y_pred.cpu().numpy()\n",
        "                \n",
        "                y_pred = list(chain.from_iterable(repeat(n, 6) for n in y_pred))\n",
        "                predictions.append(np.array(y_pred))\n",
        "          \n",
        "          predictions = np.array(predictions)\n",
        "          predictions = predictions.reshape(predictions.shape[0]*predictions.shape[1],6)\n",
        "          print(predictions.shape)\n",
        "          print(len(original_labels))\n",
        "          \n",
        "          while len(original_labels) > len(predictions):\n",
        "            original_labels = original_labels[:-1]\n",
        "          while len(original_labels) < len(predictions):\n",
        "            predictions = predictions[:-1]\n",
        "          print(len(predictions))\n",
        "          print(len(original_labels))\n",
        "          score.append(evaluate_average_percision(label_to_one_hot(original_labels).numpy(),predictions,6))\n",
        "          del batch\n",
        "         \n",
        "          torch.cuda.empty_cache()\n",
        "        session_id += 1\n",
        "        if session_id > 3:\n",
        "          subject_id += 1\n",
        "          session_id = 1\n",
        "\n",
        "    avg_precision =0\n",
        "    for i in range(len(score)):\n",
        "           avg_precision += score[i][2]['micro']\n",
        "    avg_precision = avg_precision / len(score)\n",
        "                             \n",
        "    valid_accu = int(valid_correct) / valid_count\n",
        "    print('Tess Results: ', int(valid_correct), ' out of: ',valid_count)\n",
        "    print('Accuracy: ',valid_accu)\n",
        "    print('Loss', str(val_loss / valid_count))\n",
        "    print('average precision: ',avg_precision)\n",
        "    return score\n",
        "  \n",
        "\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a_uZSwSTxdJV",
        "colab_type": "text"
      },
      "source": [
        "## Full Model Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YejcTDMY-vd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_data_MS(videos_path,label_path, scaling_factor = 0.5,sample_labels = True ):\n",
        "  \n",
        "  \n",
        "  # data  , length= load_video_MS(videos_path,scaling_factor )\n",
        "  print('Loading ',videos_path)\n",
        "  data = np.load(videos_path,allow_pickle=True)\n",
        "  length = len(data) *12\n",
        "  # print('loaded ',path)\n",
        "  # labels = np.load(labels_path,allow_pickle= True)\n",
        "  labels = get_video_label_MS(label_path,length,half = True)\n",
        "  if sample_labels:\n",
        "    # Since each chunk of video is 6 Frames, we sample the Labels at 6 labels per sample\n",
        "    labels = labels[[i for i in range(0,len(labels),6)]]\n",
        "    while (len(labels) > len(data)):\n",
        "      labels = labels[:-1]\n",
        "      print('removed from labels')\n",
        "    while (len(labels) < len(data)):\n",
        "      labels.append(labels[len(labels)-1])\n",
        "      print('Added to Data')\n",
        "\n",
        "  labels = torch.tensor(labels)\n",
        "  print(len(labels))\n",
        "  count =len(data)\n",
        "  return data , labels, count\n",
        "\n",
        "  ## Transform the Data\n",
        "  labels = torch.tensor(labels)\n",
        "  print(len(labels))\n",
        "  count =len(data)\n",
        "  # print(labels.shape)\n",
        "  # print(data.shape)\n",
        "  ## Sometime labels lenght might not be equal frames lenght,\n",
        "\n",
        "    \n",
        "def get_video_label_MS(label_path,video_lenght, half = True, data_from_numpy = False):\n",
        "  \n",
        "\n",
        "  \n",
        "  label_data = scipy.io.loadmat(label_path)\n",
        "  # Initially each frame is label 0 \n",
        "  label = [0 for i in range(video_lenght)] \n",
        "  label = np.array(label)\n",
        "\n",
        "  for category_number in range(5):\n",
        "      # Each video chunk in the same class defined as in labels\n",
        "      for video_chunk in label_data['tlabs'][category_number][0]:\n",
        "        label[video_chunk[0]:video_chunk[1]] = category_number+1\n",
        "\n",
        "  label = np.array(label)\n",
        "  if half:\n",
        "      label = label[[i for i in range(1,video_lenght,2)]]\n",
        "\n",
        "  return (label)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_video_MS(video_path, resize_scale = 0.5, half = True):\n",
        "     \n",
        "          video  = []\n",
        "          print('Loading ',video_path)\n",
        "     \n",
        "          cap = cv2.VideoCapture(video_path)\n",
        "          length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "          frame_width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
        "          frame_height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
        "          print('Frame width: '+str(frame_width) + \"   frame_height: \"+str(frame_height))\n",
        "          width ,height= int(frame_width * resize_scale), int(frame_height*resize_scale)\n",
        "          print('resizing_width: '+str(width) + '    resizing_heighth: '+str(height))\n",
        "          # taking the Background from 2nd second \n",
        "          cap.set(cv2.CAP_PROP_POS_MSEC,2*1000)\n",
        "          reval, background = cap.read()\n",
        "          # Preprocess the Background\n",
        "          background = cv2.resize(background, (width, height))\n",
        "          background = cv2.GaussianBlur(background, (21,21), 0) \n",
        "          background = cv2.cvtColor(background, cv2.COLOR_BGR2GRAY)\n",
        "          # Get back to the 0 second\n",
        "          cap.set(cv2.CAP_PROP_POS_MSEC,0 * 1000)\n",
        "          while True:\n",
        "#                 video.append(img)\n",
        "              chunk = []\n",
        "              for i in range(0,6):\n",
        "                  if half:\n",
        "                    reval, img = cap.read() # This is neglected because we load at 15 FPS , and video is stored at 30 FPS\n",
        "                  reval, img = cap.read()\n",
        "\n",
        "                  if not reval:\n",
        "                      # End of frames\n",
        "                      if i != 0:\n",
        "                        while i < 6 :\n",
        "                          chunk.append(chunk[len(chunk)-1])\n",
        "                          i +=1\n",
        "                          print(len(chunk))\n",
        "                        print('END')\n",
        "                        chunk = np.array(chunk)\n",
        "                        video.append(process_chunk(chunk,background))\n",
        "                      video = np.array(video)\n",
        "                      cap.release()\n",
        "                      del cap\n",
        "                      print(length)\n",
        "                      return video , length\n",
        "\n",
        "                  img = cv2.resize(img, (width, height))\n",
        "                  img = cv2.GaussianBlur(img,(3,3),0)\n",
        "                  chunk.append(img) # Here we store video chunk of 6 Frames and then pass it to get_stacked_pixel_trajectory\n",
        "              chunk = np.array(chunk)\n",
        "              video.append(process_chunk(chunk,background))\n",
        "          video = np.array(video)\n",
        "          cap.release()\n",
        "          del cap\n",
        "          print(length)\n",
        "          return video , length\n",
        "\n",
        "\n",
        "def process_chunk(chunk,background):\n",
        "  img1 = cv2.resize( chunk[2],(256,256))\n",
        "  img2 = get_stacked_pixel_trajectory(chunk,2)\n",
        "  img3 = cv2.resize(subtract_background(chunk[2],background),(256,256))\n",
        "  img4 = get_stacked_pixel_trajectory(subtract_background(chunk,background,List = True),2,rgb = True)\n",
        "  chunk = [img1,img2,img3,img4]\n",
        "  return chunk\n",
        "\n",
        "def get_data_loader(video,labels,batch_size, with_labels  = True):\n",
        "  x1 = [torch.tensor(video[i][0]) for i in range(len(video))]\n",
        "  x1 = [Image.fromarray(i.numpy()) for i in x1]\n",
        "  x1 = [prepare(i) for i in x1]\n",
        "  x1 = torch.stack(x1, dim = 0)\n",
        "\n",
        "  x2 = [torch.tensor(video[i][1]) for i in range(len(video))]\n",
        "  x2 = [Image.fromarray(i.numpy()) for i in x2]\n",
        "  x2 = [prepare(i) for i in x2]\n",
        "  x2 = torch.stack(x2, dim = 0)\n",
        "\n",
        "  x3 = [torch.tensor(video[i][2]) for i in range(len(video))]\n",
        "  x3 = [Image.fromarray(i.numpy()) for i in x3]\n",
        "  x3 = [prepare(i) for i in x3]\n",
        "  x3 = torch.stack(x3, dim = 0)\n",
        "\n",
        "  x4 = [torch.tensor(video[i][3]) for i in range(len(video))]\n",
        "  x4 = [Image.fromarray(i.numpy()) for i in x4]\n",
        "  x4 = [prepare(i) for i in x4]\n",
        "  x4 = torch.stack(x4, dim = 0)\n",
        "  \n",
        "  \n",
        "  \n",
        "  if with_labels :\n",
        "    my_dataset = utils.TensorDataset(x1,x2,x3,x4,labels) # create your datset\n",
        "  else:\n",
        "     my_dataset = utils.TensorDataset(x1,x2,x3,x4)   \n",
        "  dataloader = utils.DataLoader(my_dataset,batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "  return dataloader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHGn69emGGgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_fit_MS(data_path, label_path,  no_training_subjects, no_validation_subjects, batch_size,classification_model,save_path  ,\n",
        "          optimizer, no_epochs=5 ,fps=15 ,criterion =  nn.CrossEntropyLoss()):\n",
        "    '''\n",
        "    This function performs the following\n",
        "    1- load the Dataset (call get_data) according to the Type of model and Type of Data set (NUMPY of Video)\n",
        "    2- Construct the Data Loader \n",
        "    3- Perform training step (call train function)\n",
        "    4- Perform validation step\n",
        "    Parameters:\n",
        "    from_numpy (boolean) if true, the data is loaded from numpy arrays\n",
        "    '''\n",
        "    val_acc = -999\n",
        "    history = []\n",
        "    validation_subjects = no_training_subjects+1 + no_validation_subjects\n",
        "    session_id, subject_id = 1,1\n",
        "    for i in range(no_epochs):\n",
        "        print('Training Epoch {} ...'.format(i+1))\n",
        "        running_loss,train_correct,valid_correct,val_loss = 0,0,0,0\n",
        "        \n",
        "        train_count,valid_count = 0,0\n",
        "        while subject_id < no_training_subjects+1:\n",
        "\n",
        "            # Loading the videos and labels one at a time from the stored Numpy files\n",
        "            path =  data_path + \"/Copy of \"+ str(subject_id) + \"_\" + str(session_id)+ '.npy'\n",
        "            if  os.path.exists(path): # check whether this file exists in the directory\n",
        "              labels_path =  label_path + str(subject_id) +\"_\" + str(session_id)+'_label.mat'\n",
        "              data , labels, count = get_data_MS(path,labels_path )\n",
        "              train_count += count\n",
        "             \n",
        "             \n",
        "              train_dataloader = get_data_loader(data,labels,batch_size=batch_size)\n",
        "              # del data, labels, my_dataset # To free some CUDA Memory\n",
        "              # Training Step\n",
        "              loss , correct = training_batch(classification_model,train_dataloader,optimizer,criterion)\n",
        "              torch.cuda.empty_cache()\n",
        "              running_loss +=loss\n",
        "              train_correct += correct\n",
        "            else:\n",
        "                print('No such path:' ,path)\n",
        "            # Next Video    \n",
        "            session_id += 1\n",
        "            if session_id > 3:\n",
        "              subject_id += 1\n",
        "              session_id = 1\n",
        "              if subject_id > 10:\n",
        "                 torch.save(classification_model.state_dict(), save_path)\n",
        "         \n",
        "           \n",
        "        ## Validation step\n",
        "        print( \" Validating Epoch \", i+1)\n",
        "        subject_id,session_id = no_training_subjects+1 , 1\n",
        "        \n",
        "        while subject_id < validation_subjects:\n",
        "\n",
        "            # Loading the videos and labels one at a time from the stored Numpy files or from Stored Videos\n",
        "            path =  data_path + \"/Copy of \"+ str(subject_id) + \"_\" + str(session_id)+ '.npy'\n",
        "            if  os.path.exists(path): # check whether this file exists in the directory\n",
        "              labels_path =  label_path + str(subject_id) +\"_\" + str(session_id)+\"_label.mat\"\n",
        "              \n",
        "              data , labels, count = get_data_MS(path,labels_path)\n",
        "              valid_count += count\n",
        "              train_dataloader = get_data_loader(data,labels,batch_size=batch_size)\n",
        "              # del data, labels, my_dataset # To free some CUDA Memory\n",
        "              # Validation Step\n",
        "              loss , correct = validation_step(classification_model,train_dataloader,criterion)\n",
        "              torch.cuda.empty_cache()\n",
        "\n",
        "              val_loss += loss\n",
        "              valid_correct += correct\n",
        "            # Next Video\n",
        "            session_id += 1\n",
        "            if session_id > 3:\n",
        "              subject_id += 1\n",
        "              session_id = 1\n",
        "        \n",
        "        test_accu =  int(train_correct) / train_count\n",
        "        valid_accu = int(valid_correct) / valid_count\n",
        "\n",
        "        print('Train correct: ',int(train_correct), ' out of: ',train_count )\n",
        "        print('Tets correct: ', int(valid_correct), ' out of: ',valid_count)\n",
        "        print ('Epoch [{}/{}], -T-Loss : {:.6f} , Train_accuracy: {:.4f} ,  Val_loss: {:.6f}  -Val_acc: {:.4f}'.format(\n",
        "            i+1, no_epochs,running_loss/train_count,test_accu ,val_loss/valid_count, valid_accu))\n",
        "\n",
        "        if val_acc < valid_accu:\n",
        "          torch.save(classification_model.state_dict(), save_path)\n",
        "          print('val_acc has improved from {:.4f} to {:.4f}, model is saved at {}'.format(val_acc , valid_accu,save_path))\n",
        "          val_acc = valid_accu\n",
        "        else:\n",
        "          print('val_acc has not Improved from {:.4f}'.format(val_acc))    \n",
        "        history.append([i,running_loss/train_count,valid_accu,val_loss/valid_count])\n",
        "        session_id, subject_id = 1,1\n",
        "        #accu.append(100 * int(test_correct) / len(X_test))\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "\n",
        "def training_batch(classification_model,train_dataloader,optimizer,criterion):\n",
        "      running_loss,train_correct = 0,0\n",
        "      probability = nn.Softmax()\n",
        "      for x1,x2,x3,x4,y_true in train_dataloader:\n",
        "          y_true= y_true.type(torch.LongTensor)\n",
        "          y_true = y_true.cuda()\n",
        "          optimizer.zero_grad()\n",
        "          \n",
        "          x = {1:x1.cuda() , 2:x2.cuda() , 3:x3.cuda() , 4:x4.cuda()}\n",
        "          y_pred = classification_model(x)\n",
        "          del x\n",
        "          \n",
        "          torch.cuda.empty_cache()\n",
        "          y_pred = y_pred.view(-1,6)\n",
        "          loss = criterion(y_pred, y_true)\n",
        "          running_loss += loss.item()\n",
        "          y_pred = probability(y_pred)\n",
        "          train_correct +=  (y_true == y_pred.max(1)[1]).sum()\n",
        "          del y_pred\n",
        "          \n",
        "          torch.cuda.empty_cache()\n",
        "          \n",
        "          loss.backward()\n",
        "          del loss\n",
        "          optimizer.step()\n",
        "          \n",
        "      return running_loss,train_correct\n",
        "\n",
        "def validation_step(classification_model,train_dataloader,criterion):\n",
        "  val_loss,valid_correct = 0,0\n",
        "  probability = nn.Softmax()\n",
        "  for x1,x2,x3,x4,y_true in train_dataloader:\n",
        "      y_true= y_true.type(torch.LongTensor)\n",
        "      y_true = y_true.cuda()\n",
        "      \n",
        "      with torch.no_grad():\n",
        "        \n",
        "        x = {1:x1.cuda() , 2:x2.cuda() , 3:x3.cuda() , 4:x4.cuda()}\n",
        "        \n",
        "        \n",
        "        y_pred = classification_model.forward(x)\n",
        "        del x\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        y_pred = y_pred.view(-1,6)\n",
        "        loss = criterion(y_pred, y_true)\n",
        "        val_loss += loss.item()\n",
        "        y_pred = probability(y_pred)\n",
        "        valid_correct +=  (y_true == y_pred.max(1)[1]).sum()\n",
        "        torch.cuda.empty_cache()\n",
        "  return val_loss, valid_correct\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQqx5jRuQOM_",
        "colab_type": "text"
      },
      "source": [
        "## Models Training \n",
        "\n",
        "Difference between training base models is the parameters of the Training functions and the\n",
        "model.\n",
        "#####  For the spatial models: (Type 1)\n",
        "* Data is loaded form videos, so data-from_numpy is False \n",
        "* We don't need Object_trajectory \n",
        "* For the background subtraction model, we need background_sub to be True\n",
        "* Features_Extraction output = 32768 \n",
        "\n",
        "##### For the Temporal models: (Type 2)\n",
        "* Data is loaded from Numpy files, so data_from_numpy is True \n",
        "* Object_trajectory is True \n",
        "* For the background subtraction temporal model, we need background_sub to be True\n",
        "* Features_Extraction output = 150528\n",
        "\n",
        "##### Spatial background subtraction model: (Type 3)\n",
        "* Same as Type 1\n",
        "\n",
        "##### Temporal background Subtraction model: (Type 4)\n",
        "* Same as Type 2\n",
        "\n",
        "##### For the Full MSBL Model :\n",
        "* Feature_Extraction output = 366592\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw9I9DmGU5za",
        "colab_type": "code",
        "outputId": "72cb79b3-5421-4ac3-8cb6-bbf0a237f1de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#\n",
        "\n",
        "# del msbl_model\n",
        "# # torch.cuda.empty_cache()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "path_to_weights = 'drive/My Drive/Neurons/MERL Dataset/'\n",
        "model1 = backbone_feature_extractor( features_extraction_output = 32768,classification = False)\n",
        "model2 = backbone_feature_extractor( features_extraction_output = 150528,classification = False)\n",
        "model3 = backbone_feature_extractor( features_extraction_output = 32768,classification = False)\n",
        "model4 = backbone_feature_extractor( features_extraction_output = 150528,classification = False)\n",
        "model1, model2 = prepare_models(model1, model2, model3, model4,path_to_weights)\n",
        "torch.cuda.empty_cache()\n",
        "msn = MSN(spatial =model1, temporal=model2, spatial2=model3, temporal2=model4,        \n",
        "          features_extraction_output=183296)\n",
        "msbl_model = bi_lstm(msn,batch_size = 20,num_layers = 1)\n",
        "# print(msbl_model)\n",
        "torch.cuda.empty_cache()\n",
        "weights = torch.load('drive/My Drive/MERL Dataset/Full Model2.pi')\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "msbl_model.load_state_dict(weights)\n",
        "del weights \n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "msbl_model.to(device)\n",
        "data_path = 'drive/My Drive/MERL Dataset/Data/'\n",
        "labels_path = 'drive/My Drive/MERL Dataset/Labels_MERL_Shopping_Dataset/'\n",
        "\n",
        "save_path = 'drive/My Drive/MERL Dataset/Full Model2.pi'\n",
        "optimizer = create_optimizer(msbl_model,r =  0.001)  \n",
        "history = model_fit_MS(data_path=data_path ,label_path=labels_path,batch_size=20,no_training_subjects=21,\n",
        "                    no_validation_subjects=7,no_epochs=5,classification_model=msbl_model, \n",
        "                    save_path = save_path,optimizer= optimizer)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Params to learn:\n",
            "\t msn_feature_extractor.fin_vec.0.weight\n",
            "\t msn_feature_extractor.fin_vec.0.bias\n",
            "\t msn_feature_extractor.fin_vec.3.weight\n",
            "\t msn_feature_extractor.fin_vec.3.bias\n",
            "\t lstm.weight_ih_l0\n",
            "\t lstm.weight_hh_l0\n",
            "\t lstm.bias_ih_l0\n",
            "\t lstm.bias_hh_l0\n",
            "\t lstm.weight_ih_l0_reverse\n",
            "\t lstm.weight_hh_l0_reverse\n",
            "\t lstm.bias_ih_l0_reverse\n",
            "\t lstm.bias_hh_l0_reverse\n",
            "\t forward_classifier.weight\n",
            "\t forward_classifier.bias\n",
            "\t backward_classifier.weight\n",
            "\t backward_classifier.bias\n",
            "Training Epoch 1 ...\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 1_1.npy\n",
            "327\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:114: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 1_2.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 1_3.npy\n",
            "442\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 2_1.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 2_2.npy\n",
            "316\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 2_3.npy\n",
            "299\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 3_1.npy\n",
            "308\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 3_2.npy\n",
            "312\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 3_3.npy\n",
            "293\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 4_1.npy\n",
            "331\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 4_2.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 4_3.npy\n",
            "315\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 5_1.npy\n",
            "311\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 5_2.npy\n",
            "330\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 5_3.npy\n",
            "353\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 6_1.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 6_2.npy\n",
            "326\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 6_3.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 7_1.npy\n",
            "321\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 7_2.npy\n",
            "327\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 7_3.npy\n",
            "327\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 8_1.npy\n",
            "303\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 8_2.npy\n",
            "405\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 8_3.npy\n",
            "357\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 9_1.npy\n",
            "312\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 9_2.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 9_3.npy\n",
            "330\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 10_1.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 10_2.npy\n",
            "356\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 10_3.npy\n",
            "350\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 11_1.npy\n",
            "313\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 11_2.npy\n",
            "399\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 11_3.npy\n",
            "316\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 12_1.npy\n",
            "316\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 12_2.npy\n",
            "352\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 12_3.npy\n",
            "325\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 13_1.npy\n",
            "334\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 13_2.npy\n",
            "388\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 13_3.npy\n",
            "340\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 14_1.npy\n",
            "372\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 14_2.npy\n",
            "352\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 14_3.npy\n",
            "339\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 15_1.npy\n",
            "325\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 15_2.npy\n",
            "329\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 15_3.npy\n",
            "328\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 16_1.npy\n",
            "357\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 16_2.npy\n",
            "360\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 16_3.npy\n",
            "391\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 17_1.npy\n",
            "329\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 17_2.npy\n",
            "347\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 17_3.npy\n",
            "376\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 18_1.npy\n",
            "325\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 18_2.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 18_3.npy\n",
            "338\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 19_1.npy\n",
            "326\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 19_2.npy\n",
            "333\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 19_3.npy\n",
            "328\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 20_1.npy\n",
            "344\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 20_2.npy\n",
            "428\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 20_3.npy\n",
            "405\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 21_1.npy\n",
            "350\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 21_2.npy\n",
            "344\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 21_3.npy\n",
            "363\n",
            " Validating Epoch  1\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 22_1.npy\n",
            "367\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:145: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 22_2.npy\n",
            "357\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 22_3.npy\n",
            "351\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 23_1.npy\n",
            "349\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 23_2.npy\n",
            "441\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 23_3.npy\n",
            "514\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 24_1.npy\n",
            "353\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 24_2.npy\n",
            "392\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 24_3.npy\n",
            "407\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 25_1.npy\n",
            "363\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 25_2.npy\n",
            "372\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 25_3.npy\n",
            "340\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 26_1.npy\n",
            "376\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 26_2.npy\n",
            "397\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 26_3.npy\n",
            "340\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 27_1.npy\n",
            "339\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 27_2.npy\n",
            "341\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 27_3.npy\n",
            "339\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 28_1.npy\n",
            "336\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 28_2.npy\n",
            "365\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 28_3.npy\n",
            "331\n",
            "Train correct:  13837  out of:  21383\n",
            "Tets correct:  4652  out of:  7770\n",
            "Epoch [1/5], -T-Loss : 0.043787 , Train_accuracy: 0.6471 ,  Val_loss: 0.050902  -Val_acc: 0.5987\n",
            "val_acc has improved from -999.0000 to 0.5987, model is saved at drive/My Drive/MERL Dataset/Full Model2.pi\n",
            "Training Epoch 2 ...\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 1_1.npy\n",
            "327\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 1_2.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 1_3.npy\n",
            "442\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 2_1.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 2_2.npy\n",
            "316\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 2_3.npy\n",
            "299\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 3_1.npy\n",
            "308\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 3_2.npy\n",
            "312\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 3_3.npy\n",
            "293\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 4_1.npy\n",
            "331\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 4_2.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 4_3.npy\n",
            "315\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 5_1.npy\n",
            "311\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 5_2.npy\n",
            "330\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 5_3.npy\n",
            "353\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 6_1.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 6_2.npy\n",
            "326\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 6_3.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 7_1.npy\n",
            "321\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 7_2.npy\n",
            "327\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 7_3.npy\n",
            "327\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 8_1.npy\n",
            "303\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 8_2.npy\n",
            "405\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 8_3.npy\n",
            "357\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 9_1.npy\n",
            "312\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 9_2.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 9_3.npy\n",
            "330\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 10_1.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 10_2.npy\n",
            "356\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 10_3.npy\n",
            "350\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 11_1.npy\n",
            "313\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 11_2.npy\n",
            "399\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 11_3.npy\n",
            "316\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 12_1.npy\n",
            "316\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 12_2.npy\n",
            "352\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 12_3.npy\n",
            "325\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 13_1.npy\n",
            "334\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 13_2.npy\n",
            "388\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 13_3.npy\n",
            "340\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 14_1.npy\n",
            "372\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 14_2.npy\n",
            "352\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 14_3.npy\n",
            "339\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 15_1.npy\n",
            "325\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 15_2.npy\n",
            "329\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 15_3.npy\n",
            "328\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 16_1.npy\n",
            "357\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 16_2.npy\n",
            "360\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 16_3.npy\n",
            "391\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 17_1.npy\n",
            "329\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 17_2.npy\n",
            "347\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 17_3.npy\n",
            "376\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 18_1.npy\n",
            "325\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 18_2.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 18_3.npy\n",
            "338\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 19_1.npy\n",
            "326\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 19_2.npy\n",
            "333\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 19_3.npy\n",
            "328\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 20_1.npy\n",
            "344\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 20_2.npy\n",
            "428\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 20_3.npy\n",
            "405\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 21_1.npy\n",
            "350\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 21_2.npy\n",
            "344\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 21_3.npy\n",
            "363\n",
            " Validating Epoch  2\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 22_1.npy\n",
            "367\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 22_2.npy\n",
            "357\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 22_3.npy\n",
            "351\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 23_1.npy\n",
            "349\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 23_2.npy\n",
            "441\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 23_3.npy\n",
            "514\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 24_1.npy\n",
            "353\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 24_2.npy\n",
            "392\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 24_3.npy\n",
            "407\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 25_1.npy\n",
            "363\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 25_2.npy\n",
            "372\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 25_3.npy\n",
            "340\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 26_1.npy\n",
            "376\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 26_2.npy\n",
            "397\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 26_3.npy\n",
            "340\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 27_1.npy\n",
            "339\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 27_2.npy\n",
            "341\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 27_3.npy\n",
            "339\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 28_1.npy\n",
            "336\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 28_2.npy\n",
            "365\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 28_3.npy\n",
            "331\n",
            "Train correct:  14095  out of:  21383\n",
            "Tets correct:  4659  out of:  7770\n",
            "Epoch [2/5], -T-Loss : 0.042447 , Train_accuracy: 0.6592 ,  Val_loss: 0.050134  -Val_acc: 0.5996\n",
            "val_acc has improved from 0.5987 to 0.5996, model is saved at drive/My Drive/MERL Dataset/Full Model2.pi\n",
            "Training Epoch 3 ...\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 1_1.npy\n",
            "327\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 1_2.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 1_3.npy\n",
            "442\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 2_1.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 2_2.npy\n",
            "316\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 2_3.npy\n",
            "299\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 3_1.npy\n",
            "308\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 3_2.npy\n",
            "312\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 3_3.npy\n",
            "293\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 4_1.npy\n",
            "331\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 4_2.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 4_3.npy\n",
            "315\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 5_1.npy\n",
            "311\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 5_2.npy\n",
            "330\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 5_3.npy\n",
            "353\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 6_1.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 6_2.npy\n",
            "326\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 6_3.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 7_1.npy\n",
            "321\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 7_2.npy\n",
            "327\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 7_3.npy\n",
            "327\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 8_1.npy\n",
            "303\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 8_2.npy\n",
            "405\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 8_3.npy\n",
            "357\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 9_1.npy\n",
            "312\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 9_2.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 9_3.npy\n",
            "330\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 10_1.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 10_2.npy\n",
            "356\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 10_3.npy\n",
            "350\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 11_1.npy\n",
            "313\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 11_2.npy\n",
            "399\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 11_3.npy\n",
            "316\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 12_1.npy\n",
            "316\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 12_2.npy\n",
            "352\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 12_3.npy\n",
            "325\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 13_1.npy\n",
            "334\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 13_2.npy\n",
            "388\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 13_3.npy\n",
            "340\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 14_1.npy\n",
            "372\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 14_2.npy\n",
            "352\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 14_3.npy\n",
            "339\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 15_1.npy\n",
            "325\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 15_2.npy\n",
            "329\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 15_3.npy\n",
            "328\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 16_1.npy\n",
            "357\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 16_2.npy\n",
            "360\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 16_3.npy\n",
            "391\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 17_1.npy\n",
            "329\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 17_2.npy\n",
            "347\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 17_3.npy\n",
            "376\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 18_1.npy\n",
            "325\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 18_2.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 18_3.npy\n",
            "338\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 19_1.npy\n",
            "326\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 19_2.npy\n",
            "333\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 19_3.npy\n",
            "328\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 20_1.npy\n",
            "344\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 20_2.npy\n",
            "428\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 20_3.npy\n",
            "405\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 21_1.npy\n",
            "350\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 21_2.npy\n",
            "344\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 21_3.npy\n",
            "363\n",
            " Validating Epoch  3\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 22_1.npy\n",
            "367\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 22_2.npy\n",
            "357\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 22_3.npy\n",
            "351\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 23_1.npy\n",
            "349\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 23_2.npy\n",
            "441\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 23_3.npy\n",
            "514\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 24_1.npy\n",
            "353\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 24_2.npy\n",
            "392\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 24_3.npy\n",
            "407\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 25_1.npy\n",
            "363\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 25_2.npy\n",
            "372\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 25_3.npy\n",
            "340\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 26_1.npy\n",
            "376\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 26_2.npy\n",
            "397\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 26_3.npy\n",
            "340\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 27_1.npy\n",
            "339\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 27_2.npy\n",
            "341\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 27_3.npy\n",
            "339\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 28_1.npy\n",
            "336\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 28_2.npy\n",
            "365\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 28_3.npy\n",
            "331\n",
            "Train correct:  14271  out of:  21383\n",
            "Tets correct:  4723  out of:  7770\n",
            "Epoch [3/5], -T-Loss : 0.041412 , Train_accuracy: 0.6674 ,  Val_loss: 0.049753  -Val_acc: 0.6079\n",
            "val_acc has improved from 0.5996 to 0.6079, model is saved at drive/My Drive/MERL Dataset/Full Model2.pi\n",
            "Training Epoch 4 ...\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 1_1.npy\n",
            "327\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 1_2.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 1_3.npy\n",
            "442\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 2_1.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 2_2.npy\n",
            "316\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 2_3.npy\n",
            "299\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 3_1.npy\n",
            "308\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 3_2.npy\n",
            "312\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 3_3.npy\n",
            "293\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 4_1.npy\n",
            "331\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 4_2.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 4_3.npy\n",
            "315\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 5_1.npy\n",
            "311\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 5_2.npy\n",
            "330\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 5_3.npy\n",
            "353\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 6_1.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 6_2.npy\n",
            "326\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 6_3.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 7_1.npy\n",
            "321\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 7_2.npy\n",
            "327\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 7_3.npy\n",
            "327\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 8_1.npy\n",
            "303\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 8_2.npy\n",
            "405\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 8_3.npy\n",
            "357\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 9_1.npy\n",
            "312\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 9_2.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 9_3.npy\n",
            "330\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 10_1.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 10_2.npy\n",
            "356\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 10_3.npy\n",
            "350\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 11_1.npy\n",
            "313\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 11_2.npy\n",
            "399\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 11_3.npy\n",
            "316\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 12_1.npy\n",
            "316\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 12_2.npy\n",
            "352\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 12_3.npy\n",
            "325\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 13_1.npy\n",
            "334\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 13_2.npy\n",
            "388\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 13_3.npy\n",
            "340\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 14_1.npy\n",
            "372\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 14_2.npy\n",
            "352\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 14_3.npy\n",
            "339\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 15_1.npy\n",
            "325\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 15_2.npy\n",
            "329\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 15_3.npy\n",
            "328\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 16_1.npy\n",
            "357\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 16_2.npy\n",
            "360\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 16_3.npy\n",
            "391\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 17_1.npy\n",
            "329\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 17_2.npy\n",
            "347\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 17_3.npy\n",
            "376\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 18_1.npy\n",
            "325\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 18_2.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 18_3.npy\n",
            "338\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 19_1.npy\n",
            "326\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 19_2.npy\n",
            "333\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 19_3.npy\n",
            "328\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 20_1.npy\n",
            "344\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 20_2.npy\n",
            "428\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 20_3.npy\n",
            "405\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 21_1.npy\n",
            "350\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 21_2.npy\n",
            "344\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 21_3.npy\n",
            "363\n",
            " Validating Epoch  4\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 22_1.npy\n",
            "367\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 22_2.npy\n",
            "357\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 22_3.npy\n",
            "351\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 23_1.npy\n",
            "349\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 23_2.npy\n",
            "441\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 23_3.npy\n",
            "514\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 24_1.npy\n",
            "353\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 24_2.npy\n",
            "392\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 24_3.npy\n",
            "407\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 25_1.npy\n",
            "363\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 25_2.npy\n",
            "372\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 25_3.npy\n",
            "340\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 26_1.npy\n",
            "376\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 26_2.npy\n",
            "397\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 26_3.npy\n",
            "340\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 27_1.npy\n",
            "339\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 27_2.npy\n",
            "341\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 27_3.npy\n",
            "339\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 28_1.npy\n",
            "336\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 28_2.npy\n",
            "365\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 28_3.npy\n",
            "331\n",
            "Train correct:  14420  out of:  21383\n",
            "Tets correct:  4718  out of:  7770\n",
            "Epoch [4/5], -T-Loss : 0.040421 , Train_accuracy: 0.6744 ,  Val_loss: 0.049886  -Val_acc: 0.6072\n",
            "val_acc has not Improved from 0.6079\n",
            "Training Epoch 5 ...\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 1_1.npy\n",
            "327\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 1_2.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 1_3.npy\n",
            "442\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 2_1.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 2_2.npy\n",
            "316\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 2_3.npy\n",
            "299\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 3_1.npy\n",
            "308\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 3_2.npy\n",
            "312\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 3_3.npy\n",
            "293\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 4_1.npy\n",
            "331\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 4_2.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 4_3.npy\n",
            "315\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 5_1.npy\n",
            "311\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 5_2.npy\n",
            "330\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 5_3.npy\n",
            "353\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 6_1.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 6_2.npy\n",
            "326\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 6_3.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 7_1.npy\n",
            "321\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 7_2.npy\n",
            "327\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 7_3.npy\n",
            "327\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 8_1.npy\n",
            "303\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 8_2.npy\n",
            "405\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 8_3.npy\n",
            "357\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 9_1.npy\n",
            "312\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 9_2.npy\n",
            "323\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 9_3.npy\n",
            "330\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 10_1.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 10_2.npy\n",
            "356\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 10_3.npy\n",
            "350\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 11_1.npy\n",
            "313\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 11_2.npy\n",
            "399\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 11_3.npy\n",
            "316\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 12_1.npy\n",
            "316\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 12_2.npy\n",
            "352\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 12_3.npy\n",
            "325\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 13_1.npy\n",
            "334\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 13_2.npy\n",
            "388\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 13_3.npy\n",
            "340\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 14_1.npy\n",
            "372\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 14_2.npy\n",
            "352\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 14_3.npy\n",
            "339\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 15_1.npy\n",
            "325\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 15_2.npy\n",
            "329\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 15_3.npy\n",
            "328\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 16_1.npy\n",
            "357\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 16_2.npy\n",
            "360\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 16_3.npy\n",
            "391\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 17_1.npy\n",
            "329\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 17_2.npy\n",
            "347\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 17_3.npy\n",
            "376\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 18_1.npy\n",
            "325\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 18_2.npy\n",
            "317\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 18_3.npy\n",
            "338\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 19_1.npy\n",
            "326\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 19_2.npy\n",
            "333\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 19_3.npy\n",
            "328\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 20_1.npy\n",
            "344\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 20_2.npy\n",
            "428\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 20_3.npy\n",
            "405\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 21_1.npy\n",
            "350\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 21_2.npy\n",
            "344\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 21_3.npy\n",
            "363\n",
            " Validating Epoch  5\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 22_1.npy\n",
            "367\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 22_2.npy\n",
            "357\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 22_3.npy\n",
            "351\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 23_1.npy\n",
            "349\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 23_2.npy\n",
            "441\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 23_3.npy\n",
            "514\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 24_1.npy\n",
            "353\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 24_2.npy\n",
            "392\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 24_3.npy\n",
            "407\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 25_1.npy\n",
            "363\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 25_2.npy\n",
            "372\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 25_3.npy\n",
            "340\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 26_1.npy\n",
            "376\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 26_2.npy\n",
            "397\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 26_3.npy\n",
            "340\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 27_1.npy\n",
            "339\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 27_2.npy\n",
            "341\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 27_3.npy\n",
            "339\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 28_1.npy\n",
            "336\n",
            "365\n",
            "Loading  drive/My Drive/MERL Dataset/Data//Copy of 28_3.npy\n",
            "331\n",
            "Train correct:  14611  out of:  21383\n",
            "Tets correct:  4775  out of:  7770\n",
            "Epoch [5/5], -T-Loss : 0.039416 , Train_accuracy: 0.6833 ,  Val_loss: 0.048870  -Val_acc: 0.6145\n",
            "val_acc has improved from 0.6079 to 0.6145, model is saved at drive/My Drive/MERL Dataset/Full Model2.pi\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b6Nrd8IaGiu",
        "colab_type": "text"
      },
      "source": [
        "## Test\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CwImm46Kg2Qp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_step(classification_model,train_dataloader):\n",
        "  val_loss,valid_correct = 0,0\n",
        "  probability = nn.Softmax()\n",
        "  predictions = []\n",
        "  for x1,x2,x3,x4,y_true in train_dataloader:\n",
        "      y_true= y_true.type(torch.LongTensor)\n",
        "      y_true = y_true.cuda()\n",
        "      \n",
        "      with torch.no_grad():\n",
        "        x = {1:x1.cuda() , 2:x2.cuda() , 3:x3.cuda() , 4:x4.cuda()}\n",
        "        y_pred = classification_model.forward(x)\n",
        "        del x\n",
        "        torch.cuda.empty_cache()\n",
        "        y_pred = y_pred.view(-1,6)\n",
        "        y_pred = probability(y_pred)\n",
        "        y_pred = y_pred.max(1)[1]\n",
        "        \n",
        "        valid_correct +=  (y_true == y_pred).sum()\n",
        "        torch.cuda.empty_cache()\n",
        "        y_pred = y_pred.cpu()\n",
        "        y_pred = list(chain.from_iterable(repeat(n, 6) for n in y_pred))\n",
        "        predictions.append((y_pred))\n",
        "  return test_correct, predictions\n",
        "\n",
        "def predict(classification_model,data_path, labels_path, subject_id, no_testSubjects, batch_size):\n",
        "   test_correct = 0\n",
        "   threshold = subject_id + no_testSubjects+1\n",
        "   results, predictions, gt = [],[],[]\n",
        "  \n",
        "   while subject_id < threshold:\n",
        "\n",
        "            # Loading the videos and labels one at a time from the stored Numpy files\n",
        "            path =  data_path +  str(subject_id) + \"_\" + str(session_id)+ '.npy'\n",
        "            if  os.path.exists(path): # check whether this file exists in the directory\n",
        "              labels_path =  label_path + str(subject_id) +\"_\" + str(session_id)+'_label.mat'\n",
        "              data , original_labels, count = get_data_MS(path,labels_path,sample_labels = False )\n",
        "              labels = original_labels[[i for i in range(0,len(original_labels),6)]]\n",
        "              while (len(labels) > len(data)):\n",
        "                list(labels).remove(labels[len(labels)-1])\n",
        "                print('removed from labels')\n",
        "              while (len(labels) < len(data)):\n",
        "                print('Added To labels')\n",
        "                list(labels).append(labels[len(labels)-1])\n",
        "              \n",
        "              \n",
        "              labels = torch.tensor(labels)\n",
        "             \n",
        "              train_dataloader = get_data_loader(data,labels,batch_size=batch_size)\n",
        "              correct,predictions = test_step(classification_model,train_dataloader,optimizer,criterion)\n",
        "              torch.cuda.empty_cache()\n",
        "              \n",
        "              test_correct += correct\n",
        "              predictions = np.array(predictions)\n",
        "              print(predictions.shape)\n",
        "              predictions = predictions.reshape(predictions.shape[0]*predictions.shape[1])\n",
        "              print(predictions.shape)\n",
        "              print(len(original_labels))\n",
        "              \n",
        "              while len(original_labels) > len(predictions):\n",
        "                original_labels = original_labels[:-1]\n",
        "              while len(original_labels) < len(predictions):\n",
        "                predictions = predictions[:-1]\n",
        "              print(len(predictions))\n",
        "              print(len(original_labels))\n",
        "              gt.append((original_labels))\n",
        "              results.append((predictions))\n",
        "            else:\n",
        "                print('No such path:' ,path)\n",
        "            # Next Video    \n",
        "            session_id += 1\n",
        "            if session_id > 3:\n",
        "              subject_id += 1\n",
        "              session_id = 1\n",
        "    \n",
        "    \n",
        "   gt = list(itertools.chain.from_iterable(gt))\n",
        "   gt = [i.item() for i in gt]\n",
        "   results =  list(itertools.chain.from_iterable(results))\n",
        "   mav = evaluate(results,gt)  \n",
        "   print('average precision: ',mav)\n",
        "   return mav"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJaOkrH8g3iJ",
        "colab_type": "code",
        "outputId": "5dc18df4-6f8e-4b85-8705-989533d27321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import itertools\n",
        "classification_model =msbl_model \n",
        "subject_id = 32\n",
        "session_id = 1\n",
        "no_testSubjects = 10\n",
        "batch_size = 20\n",
        "data_path = 'drive/My Drive/MERL Dataset/Data/'\n",
        "label_path = 'drive/My Drive/MERL Dataset/Labels_MERL_Shopping_Dataset/'\n",
        "test_correct = 0\n",
        "threshold = subject_id + no_testSubjects+1\n",
        "results, predictions, gt = [],[],[]\n",
        "\n",
        "while subject_id < threshold:\n",
        "\n",
        "        # Loading the videos and labels one at a time from the stored Numpy files\n",
        "        path =  data_path +  str(subject_id) + \"_\" + str(session_id)+ '.npy'\n",
        "        if  os.path.exists(path): # check whether this file exists in the directory\n",
        "          labels_path =  label_path + str(subject_id) +\"_\" + str(session_id)+'_label.mat'\n",
        "          data , original_labels, count = get_data_MS(path,labels_path, sample_labels = False )\n",
        "          \n",
        "          labels = original_labels[[i for i in range(0,len(original_labels),6)]]\n",
        "          while len(labels) > len(data):\n",
        "                labels = labels[:-1]\n",
        "          while len(labels) < len(data):\n",
        "                data = data[:-1]\n",
        "          \n",
        "          \n",
        "          labels = torch.tensor(labels)\n",
        "          \n",
        "          train_dataloader = get_data_loader(data,labels,batch_size=batch_size)\n",
        "          correct,predictions = test_step(classification_model,train_dataloader)\n",
        "          torch.cuda.empty_cache()\n",
        "          \n",
        "          test_correct += correct\n",
        "          predictions = np.array(predictions)\n",
        "          predictions = list(predictions.reshape(predictions.shape[0]*predictions.shape[1]))\n",
        "          print('Len of original labels: ',len(original_labels))\n",
        "          print('Len of  predictions: ',len(predictions))\n",
        "          while len(original_labels) > len(predictions):\n",
        "            original_labels = original_labels[:-1]\n",
        "            print('Removed from abels')\n",
        "          while len(original_labels) < len(predictions):\n",
        "            list(original_labels).append(original_labels[len(original_labels)-1])\n",
        "            print('Added to Labels')\n",
        "          print(len(predictions))\n",
        "          print(len(original_labels))\n",
        "          gt.append((original_labels))\n",
        "          predictions = np.array(predictions)\n",
        "          results.append((predictions))\n",
        "        else:\n",
        "            print('No such path:' ,path)\n",
        "        # Next Video    \n",
        "        session_id += 1\n",
        "        if session_id > 3:\n",
        "          subject_id += 1\n",
        "          session_id = 1\n",
        "\n",
        "gt = list(itertools.chain.from_iterable(gt))\n",
        "gt = [i.item() for i in gt]\n",
        "results =  list(itertools.chain.from_iterable(results))\n",
        "print(len(gt))\n",
        "print(len(results))\n",
        "mav = evaluate(results,gt)  \n",
        "print('average precision: ',mav)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading  drive/My Drive/MERL Dataset/Data/32_1.npy\n",
            "2106\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Len of original labels:  2106\n",
            "Len of  predictions:  2040\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "2040\n",
            "2040\n",
            "Loading  drive/My Drive/MERL Dataset/Data/32_2.npy\n",
            "1932\n",
            "Len of original labels:  1932\n",
            "Len of  predictions:  1920\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "1920\n",
            "1920\n",
            "Loading  drive/My Drive/MERL Dataset/Data/32_3.npy\n",
            "1902\n",
            "Len of original labels:  1902\n",
            "Len of  predictions:  1800\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "1800\n",
            "1800\n",
            "Loading  drive/My Drive/MERL Dataset/Data/33_1.npy\n",
            "1932\n",
            "Len of original labels:  1932\n",
            "Len of  predictions:  1920\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "1920\n",
            "1920\n",
            "No such path: drive/My Drive/MERL Dataset/Data/33_2.npy\n",
            "No such path: drive/My Drive/MERL Dataset/Data/33_3.npy\n",
            "Loading  drive/My Drive/MERL Dataset/Data/34_1.npy\n",
            "1998\n",
            "Len of original labels:  1998\n",
            "Len of  predictions:  1920\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "1920\n",
            "1920\n",
            "No such path: drive/My Drive/MERL Dataset/Data/34_2.npy\n",
            "No such path: drive/My Drive/MERL Dataset/Data/34_3.npy\n",
            "Loading  drive/My Drive/MERL Dataset/Data/35_1.npy\n",
            "1944\n",
            "Len of original labels:  1944\n",
            "Len of  predictions:  1920\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "1920\n",
            "1920\n",
            "No such path: drive/My Drive/MERL Dataset/Data/35_2.npy\n",
            "No such path: drive/My Drive/MERL Dataset/Data/35_3.npy\n",
            "Loading  drive/My Drive/MERL Dataset/Data/36_1.npy\n",
            "1890\n",
            "Len of original labels:  1890\n",
            "Len of  predictions:  1800\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "1800\n",
            "1800\n",
            "No such path: drive/My Drive/MERL Dataset/Data/36_2.npy\n",
            "No such path: drive/My Drive/MERL Dataset/Data/36_3.npy\n",
            "Loading  drive/My Drive/MERL Dataset/Data/37_1.npy\n",
            "2016\n",
            "Len of original labels:  2016\n",
            "Len of  predictions:  1920\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "1920\n",
            "1920\n",
            "No such path: drive/My Drive/MERL Dataset/Data/37_2.npy\n",
            "No such path: drive/My Drive/MERL Dataset/Data/37_3.npy\n",
            "Loading  drive/My Drive/MERL Dataset/Data/38_1.npy\n",
            "1968\n",
            "Len of original labels:  1968\n",
            "Len of  predictions:  1920\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "1920\n",
            "1920\n",
            "No such path: drive/My Drive/MERL Dataset/Data/38_2.npy\n",
            "No such path: drive/My Drive/MERL Dataset/Data/38_3.npy\n",
            "Loading  drive/My Drive/MERL Dataset/Data/39_1.npy\n",
            "1992\n",
            "Len of original labels:  1992\n",
            "Len of  predictions:  1920\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "1920\n",
            "1920\n",
            "No such path: drive/My Drive/MERL Dataset/Data/39_2.npy\n",
            "No such path: drive/My Drive/MERL Dataset/Data/39_3.npy\n",
            "Loading  drive/My Drive/MERL Dataset/Data/40_1.npy\n",
            "2010\n",
            "Len of original labels:  2010\n",
            "Len of  predictions:  1920\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "Removed from abels\n",
            "1920\n",
            "1920\n",
            "No such path: drive/My Drive/MERL Dataset/Data/40_2.npy\n",
            "No such path: drive/My Drive/MERL Dataset/Data/40_3.npy\n",
            "No such path: drive/My Drive/MERL Dataset/Data/41_1.npy\n",
            "No such path: drive/My Drive/MERL Dataset/Data/41_2.npy\n",
            "No such path: drive/My Drive/MERL Dataset/Data/41_3.npy\n",
            "No such path: drive/My Drive/MERL Dataset/Data/42_1.npy\n",
            "No such path: drive/My Drive/MERL Dataset/Data/42_2.npy\n",
            "No such path: drive/My Drive/MERL Dataset/Data/42_3.npy\n",
            "21000\n",
            "21000\n",
            "precision per class = {'1': 0.6936605316973415, '2': 0.6821000413393964, '3': 0.5660044150110375, '4': 0.46254980079681274, '5': 0.7274481427882297}\n",
            "mAP = 0.6263525863265635\n",
            "average precision:  0.6263525863265635\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SGho_fkJh8dO",
        "colab_type": "code",
        "outputId": "f04676de-0c6e-4480-d1f8-9f7477f39bc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(gt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3500"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlTgMItmuv_r",
        "colab_type": "text"
      },
      "source": [
        "## Output "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2PvBd61nNFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inferOnVideo(path_to_video):\n",
        "\n",
        "  torch.cuda.empty_cache()\n",
        "  path_to_weights = 'drive/My Drive/MERL Dataset/'\n",
        "  model1 = backbone_feature_extractor( features_extraction_output = 32768,classification = False)\n",
        "  model2 = backbone_feature_extractor( features_extraction_output = 150528,classification = False)\n",
        "  model3 = backbone_feature_extractor( features_extraction_output = 32768,classification = False)\n",
        "  model4 = backbone_feature_extractor( features_extraction_output = 150528,classification = False)\n",
        "  model1, model2 = prepare_models(model1, model2, model3, model4,path_to_weights)\n",
        "  torch.cuda.empty_cache()\n",
        "  msn = MSN(spatial =model1, temporal=model2, spatial2=model3, temporal2=model4,        \n",
        "            features_extraction_output=183296)\n",
        "  msbl_model = bi_lstm(msn,batch_size = 20,num_layers = 1)\n",
        "  torch.cuda.empty_cache()\n",
        "  weights = torch.load(path_to_weights + '/Full Model2.pi')\n",
        "  torch.cuda.empty_cache()\n",
        "  msbl_model.load_state_dict(weights)\n",
        "  del weights \n",
        "  torch.cuda.empty_cache()\n",
        "  msbl_model.to(device)\n",
        "  classification_model = msbl_model\n",
        "  video, length = load_video_MS(path_to_video, half = False)\n",
        "  test_dataloader = get_data_loader(video, batch_size = 20, with_labels = False)\n",
        "  predictions = []\n",
        "  probability = nn.Softmax()\n",
        "  for x1,x2,x3,x4 in test_dataloader:\n",
        "      with torch.no_grad():\n",
        "        x = {1:x1.cuda() , 2:x2.cuda() , 3:x3.cuda() , 4:x4.cuda()}\n",
        "        y_pred = classification_model.forward(x)\n",
        "        del x\n",
        "        torch.cuda.empty_cache()\n",
        "        y_pred = y_pred.view(-1,6)\n",
        "        y_pred = probability(y_pred)\n",
        "        y_pred = y_pred.max(1)[1]\n",
        "        y_pred = y_pred.cpu()\n",
        "        torch.cuda.empty_cache()\n",
        "        y_pred = list(chain.from_iterable(repeat(n, 7) for n in y_pred))\n",
        "        predictions.append((y_pred))\n",
        "  predictions = np.array(predictions)\n",
        "  predictions = list(predictions.reshape(predictions.shape[0]*predictions.shape[1]))\n",
        "\n",
        "  return predictions\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJ4AZ6x0mGci",
        "colab_type": "code",
        "outputId": "d58c1a22-0785-4b4b-d178-a14a149c648d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "video_path = 'drive/My Drive/MERL Dataset/Videos_MERL_Shopping_Dataset/10_1_crop.mp4'\n",
        "result = inferOnVideo(video_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-364c29d032db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'drive/My Drive/MERL Dataset/Videos_MERL_Shopping_Dataset/10_1_crop.mp4'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minferOnVideo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-a26064314847>\u001b[0m in \u001b[0;36minferOnVideo\u001b[0;34m(path_to_video)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackbone_feature_extractor\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfeatures_extraction_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32768\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mmodel4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackbone_feature_extractor\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mfeatures_extraction_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m150528\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclassification\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath_to_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   msn = MSN(spatial =model1, temporal=model2, spatial2=model3, temporal2=model4,        \n",
            "\u001b[0;32m<ipython-input-6-5bfbf69332aa>\u001b[0m in \u001b[0;36mprepare_models\u001b[0;34m(spatial, temporal, spatial2, temporal2, path_to_weights_dir)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprepare_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemporal\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mspatial2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemporal2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpath_to_weights_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m   \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_weights_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/model1.pi\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m   \u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_weights_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"/model_temporal.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtemporal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'drive/My Drive/MERL Dataset//model1.pi'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NxLA2CPxuyfm",
        "colab_type": "code",
        "outputId": "0e7ee5f1-33ef-4b32-9da6-43686971cc84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "  # path_to_video = 'drive/My Drive/MERL Dataset/Videos_MERL_Shopping_Dataset/10_1_crop.mp4'\n",
        "  # torch.cuda.empty_cache()\n",
        "  # path_to_weights = 'drive/My Drive/MERL Dataset/'\n",
        "  # model1 = backbone_feature_extractor( features_extraction_output = 32768,classification = False)\n",
        "  # model2 = backbone_feature_extractor( features_extraction_output = 150528,classification = False)\n",
        "  # model3 = backbone_feature_extractor( features_extraction_output = 32768,classification = False)\n",
        "  # model4 = backbone_feature_extractor( features_extraction_output = 150528,classification = False)\n",
        "  # model1, model2 = prepare_models(model1, model2, model3, model4,path_to_weights)\n",
        "  # torch.cuda.empty_cache()\n",
        "  # msn = MSN(spatial =model1, temporal=model2, spatial2=model3, temporal2=model4,        \n",
        "  #           features_extraction_output=183296)\n",
        "  # msbl_model = bi_lstm(msn,batch_size = 20,num_layers = 1)\n",
        "  # torch.cuda.empty_cache()\n",
        "  # weights = torch.load(path_to_weights + '/Full Model2.pi')\n",
        "  # torch.cuda.empty_cache()\n",
        "  # msbl_model.load_state_dict(weights)\n",
        "  # del weights \n",
        "  # torch.cuda.empty_cache()\n",
        "  # msbl_model.to(device)\n",
        "  # classification_model = msbl_model\n",
        "  # video, length = load_video_MS(path_to_video, half = False)\n",
        "  # test_dataloader = get_data_loader(video,labels = None, batch_size = 20, with_labels = False)\n",
        "  predictions = []\n",
        "  probability = nn.Softmax()\n",
        "  for x1,x2,x3,x4 in test_dataloader:\n",
        "      with torch.no_grad():\n",
        "        x = {1:x1.cuda() , 2:x2.cuda() , 3:x3.cuda() , 4:x4.cuda()}\n",
        "        y_pred = classification_model.forward(x)\n",
        "        del x\n",
        "        torch.cuda.empty_cache()\n",
        "        y_pred = y_pred.view(-1,6)\n",
        "        y_pred = probability(y_pred)\n",
        "        y_pred = y_pred.max(1)[1]\n",
        "        y_pred = y_pred.cpu()\n",
        "        torch.cuda.empty_cache()\n",
        "        y_pred = list(chain.from_iterable(repeat(n, 7) for n in y_pred))\n",
        "        predictions.append((y_pred))\n",
        "  predictions = np.array(predictions)\n",
        "  predictions = list(predictions.reshape(predictions.shape[0]*predictions.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDxLyIuQecHs",
        "colab_type": "code",
        "outputId": "ca3335d3-9192-4580-fc23-83c1aef83f67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.array(predictions).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4340,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4uilS22kPU0",
        "colab_type": "code",
        "outputId": "a83547cf-2914-42da-aaae-662379c4aa1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZEv-0IEImHS",
        "colab_type": "code",
        "outputId": "6f42323f-b409-4479-db1d-bb72e2af6eeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(356, 4)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zX7KCfv6kma0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQFYYV3UxLfR",
        "colab_type": "text"
      },
      "source": [
        "## Writing predictions on Video"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmLckhLmxN5n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract predictions\n",
        "    output_frames = []\n",
        "    for frame in video:\n",
        "        image_tensor = Variable(transform(frame)).to(device)\n",
        "        \n",
        "\n",
        "        # Get label prediction for frame\n",
        "        with torch.no_grad():\n",
        "            prediction = model(image_tensor)\n",
        "            predicted_label = labels[prediction.argmax(1).item()]\n",
        "\n",
        "        # Draw label on frame\n",
        "        d = ImageDraw.Draw(frame)\n",
        "        d.text(xy=(10, 10), text=predicted_label, fill=(255, 255, 255))\n",
        "\n",
        "        output_frames += [frame]\n",
        "\n",
        "    # Create video from frames\n",
        "    writer = skvideo.io.FFmpegWriter(\"output.gif\")\n",
        "    for frame in tqdm.tqdm(output_frames, desc=\"Writing to video\"):\n",
        "        writer.writeFrame(np.array(frame))\n",
        "    writer.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pDJqHUpPCIMx",
        "colab_type": "code",
        "outputId": "2c440387-14d9-4f87-a6bd-69e41f7040a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "path = 'drive/My Drive/MERL Dataset/Videos_MERL_Shopping_Dataset/10_2_crop.mp4'\n",
        "%time video,lenght = load_video(path,resize_scale=0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading  drive/My Drive/MERL Dataset/Videos_MERL_Shopping_Dataset/10_2_crop.mp4\n",
            "Frame width: 920.0   frame_height: 680.0\n",
            "resizing_width: 460    resizing_heighth: 340\n",
            "6\n",
            "END\n",
            "4271\n",
            "CPU times: user 5min, sys: 25.8 s, total: 5min 25s\n",
            "Wall time: 4min 40s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vp3XLFLfy-ZC",
        "colab_type": "code",
        "outputId": "e847d3b3-1709-443f-b2c5-84b38a9684b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "subject_id = 21\n",
        "session_id = 1 \n",
        "threshold = 40\n",
        "data_path ='drive/My Drive/MERL Dataset/Videos_MERL_Shopping_Dataset/'\n",
        "label_path = 'drive/My Drive/MERL Dataset/Labels_MERL_Shopping_Dataset/'\n",
        "data_save_path = 'drive/My Drive/MERL Dataset/Data_background_subtracted_OT/'\n",
        "labels_save_path = 'drive/My Drive/MERL Dataset/Labels_background_subtracted_OT numpy/'\n",
        "while subject_id < threshold:\n",
        "\n",
        "            # Loading the videos and labels one at a time from the stored Numpy files\n",
        "            \n",
        "            path =  data_path + \"/\"+ str(subject_id) + \"_\" + str(session_id)+\"_crop.mp4\"\n",
        "            if  os.path.exists(path): # check whether this file exists in the directory\n",
        "              # Function load_video depends on the model\n",
        "              # Here, we load the Data from pre-proccessed numpy files\n",
        "              data  , length= load_video(video_path = path, background_sub=  True,object_trajectory = True)\n",
        "              labels_path =  label_path +'/'+str(subject_id) +\"_\" + str(session_id)+\"_label.mat\"\n",
        "              original_labels = get_video_label(labels_path,length)\n",
        "              video_out = data_save_path +\"/\"+ str(subject_id) + \"_\" + str(session_id)+'.npy'\n",
        "              labels_out = labels_save_path +'/'+str(subject_id) +\"_\" + str(session_id)+\"_label.npy\"\n",
        "              np.save(video_out , data)\n",
        "              np.save(labels_out, original_labels)\n",
        "            else:\n",
        "              print('Path does not exist: ', path)\n",
        "            # Next Video    \n",
        "            session_id += 1\n",
        "            if session_id > 3:\n",
        "              subject_id += 1\n",
        "              session_id = 1\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-2badc8cc5592>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m               \u001b[0;31m# Function load_video depends on the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m               \u001b[0;31m# Here, we load the Data from pre-proccessed numpy files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m               \u001b[0mdata\u001b[0m  \u001b[0;34m,\u001b[0m \u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mload_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackground_sub\u001b[0m\u001b[0;34m=\u001b[0m  \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobject_trajectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m               \u001b[0mlabels_path\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mlabel_path\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_label.mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m               \u001b[0moriginal_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_video_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'load_video' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AEFIXb-jGK9",
        "colab_type": "code",
        "outputId": "561c7de7-4e85-47de-98c5-9f48fe333a38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "# gpu = GPUs[0]\n",
        "def printm():\n",
        "  GPUs = GPU.getGPUs()\n",
        "  gpu = GPUs[0]\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 25.7 GB  | Proc size: 1.6 GB\n",
            "GPU RAM Free: 11430MB | Used: 11MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}